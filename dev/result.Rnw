\documentclass[11pt]{article}
\usepackage[portrait, margin=1.5cm]{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage[dvipsnames,table]{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{amsmath}

\title{\vspace{2cm}\Huge\textbf{Backtest Results}\\\vspace{0.5cm}\Large result\_nick\_20260202\_stagger1\_f1d8}
\author{}
\date{\vspace{1cm}\today}

% Setup reticulate with project virtualenv
<<setup, include=FALSE>>=
library(reticulate)
use_virtualenv("/Users/nicknassuphis/specparser/.venv", required = TRUE)
library(knitr)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, results='asis')
@

% All Python code in one chunk to ensure variable persistence
<<all_python, engine='python', include=FALSE>>=
import sys
import os
import pandas as pd
import numpy as np
import warnings
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numba

# --- Numba helper functions ---

@numba.njit(parallel=True)
def cube_rwlf(x, f, threshold=1e-15):
    """
    Rolling with last fill.

    For each (slice, col), when f[row, col, slice] > threshold,
    grab x[row, col] and carry forward.

    Args:
        x: 2D array (n_rows, n_cols) - source values
        f: 3D array (n_rows, n_cols, n_slices) - trigger points

    Returns:
        3D array - values from x propagated from trigger points
    """
    n_rows, n_cols, n_slices = f.shape
    res = np.zeros((n_rows, n_cols, n_slices), dtype=x.dtype)

    for slice_idx in numba.prange(n_slices):
        for col in range(n_cols):
            last = 0.0
            for row in range(n_rows):
                if f[row, col, slice_idx] > threshold:
                    last = x[row, col]
                res[row, col, slice_idx] = last

    return res


def load_cube_as_3d(result_dir, cube_name):
    """Load a cube from parquet and reshape to 3D numpy array."""
    df = pd.read_parquet(f"{result_dir}/{cube_name}.parquet")
    date_df = pd.read_parquet(f"{result_dir}/{cube_name}_date.parquet")
    asset_df = pd.read_parquet(f"{result_dir}/{cube_name}_asset.parquet")

    # Get third dimension (slice, result, signal, etc.)
    dim3_col = [c for c in df.columns if c not in ['date', 'asset', 'value']][0]
    dim3_df = pd.read_parquet(f"{result_dir}/{cube_name}_{dim3_col}.parquet")

    n_dates, n_assets, n_dim3 = len(date_df), len(asset_df), len(dim3_df)
    cube = df['value'].values.reshape((n_dates, n_assets, n_dim3), order='C')

    return cube, date_df, asset_df, dim3_df


# Paths
PROJECT_ROOT = "/Users/nicknassuphis/specparser"
RESULT_DIR = PROJECT_ROOT + "/data/result_nick_20260202_stagger1_f1d8"
FIGURE_DIR = PROJECT_ROOT + "/dev/"

# --- Data Overview ---
# List all parquet files with sizes
files_info = []
for f in sorted(os.listdir(RESULT_DIR)):
    if f.endswith('.parquet'):
        size = os.path.getsize(os.path.join(RESULT_DIR, f))
        files_info.append({'file': f, 'size_mb': size / 1024 / 1024})

files_df = pd.DataFrame(files_info)
total_size_mb = files_df['size_mb'].sum()
n_files = len(files_df)

# --- Load Dimension Files ---
asset_df = pd.read_parquet(f"{RESULT_DIR}/asset.parquet")
date_df = pd.read_parquet(f"{RESULT_DIR}/date.parquet")
n_assets = len(asset_df)
n_dates = len(date_df)
date_min = date_df['date_string'].min()
date_max = date_df['date_string'].max()

# --- Load Strategy Cube Metadata ---
strategy_result = pd.read_parquet(f"{RESULT_DIR}/strategy_result.parquet")
n_strategy_slices = len(strategy_result)

# --- Load Stats ---
stats_df = pd.read_parquet(f"{RESULT_DIR}/stats.parquet")

# --- Load Signals Metadata ---
signals_signal = pd.read_parquet(f"{RESULT_DIR}/signals_signal.parquet")
n_signals = len(signals_signal)

# --- Load PnL Cube Slice Metadata ---
pnl_slice = pd.read_parquet(f"{RESULT_DIR}/pnl_cube_slice.parquet")
n_pnl_slices = len(pnl_slice)

# --- Load Reference Result Metadata ---
reference_result = pd.read_parquet(f"{RESULT_DIR}/reference_result.parquet")
n_reference_slices = len(reference_result)

# --- Cube Sizes ---
cubes_info = []
cube_names = ['strategy', 'pnl_cube', 'signals', 'residuals', 'reference',
              'signal_ranks', 'entry_cube', 'expiry_cube', 'decay_cube',
              'vega_cube', 'vol_cube', 'option_pnl_cube', 'delta_pnl_cube']

for cube in cube_names:
    fpath = f"{RESULT_DIR}/{cube}.parquet"
    if os.path.exists(fpath):
        size = os.path.getsize(fpath) / 1024 / 1024
        df = pd.read_parquet(fpath)
        cubes_info.append({
            'cube': cube,
            'rows': len(df),
            'size_mb': size
        })

cubes_df = pd.DataFrame(cubes_info)

# --- Generate Files Overview Table ---
def files_to_latex(df):
    # Split into 3 columns
    n = len(df)
    third = (n + 2) // 3

    left = df.iloc[:third].reset_index(drop=True)
    mid = df.iloc[third:2*third].reset_index(drop=True)
    right = df.iloc[2*third:].reset_index(drop=True)

    # Pad shorter columns
    max_len = max(len(left), len(mid), len(right))
    for d in [left, mid, right]:
        while len(d) < max_len:
            d.loc[len(d)] = {'file': '', 'size_mb': np.nan}

    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Parquet Files in Result Directory}')
    lines.append(r'\label{tab:files}')
    lines.append(r'\scriptsize')
    lines.append(r'\begin{tabular}{lr|lr|lr}')
    lines.append(r'\toprule')
    lines.append(r'File & MB & File & MB & File & MB \\')
    lines.append(r'\midrule')

    for i in range(max_len):
        cells = []
        for d in [left, mid, right]:
            f = d.loc[i, 'file'] if i < len(d) else ''
            s = d.loc[i, 'size_mb'] if i < len(d) else np.nan
            if pd.isna(s) or f == '':
                cells.extend(['', ''])
            else:
                # Truncate long filenames
                fname = f.replace('.parquet', '').replace('_', r'\_')
                if len(fname) > 25:
                    fname = fname[:22] + '...'
                cells.extend([fname, f'{s:.1f}'])
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

files_latex = files_to_latex(files_df)

# --- Generate File Contents Table ---
# Compact color-coded table with one-line descriptions

file_contents_data = [
    # (file, category, rows, description)
    # Dimension files
    ('asset', 'dim', 180, 'Asset index → name mapping'),
    ('date', 'dim', 9854, 'Date index → string mapping'),
    ('asset_ndx', 'dim', 180, 'Alternative asset indexing'),
    ('date_ndx', 'dim', 9854, 'Alternative date indexing'),
    ('group_names', 'dim', 5, 'Asset class names (commodity/equity/fx/rates/stonks)'),
    ('subgroup_names', 'dim', 11, 'Subgroup names (regional/style)'),

    # Config files
    ('AMT', 'config', 237, 'Asset config as JSON (Class, Hedge, Vol, Slippage, etc.)'),
    ('asset_config', 'config', 189, 'Per-asset params (group, risk, limit, cap, model)'),
    ('yaml_config', 'config', 5211, 'YAML config file lines'),
    ('json_config', 'config', 1, 'JSON config string'),
    ('run_options', 'config', 13143, 'Runtime options (git, timestamp, paths)'),

    # Strategy cube
    ('strategy', 'cube', 39021840, '3D cube: 9854×180×22 (pnl, hedged, wpnl, ranks, etc.)'),
    ('strategy_result', 'cube', 22, 'Strategy slice names'),

    # P&L cubes (all 9854×180×10)
    ('pnl_cube', 'pnl', 17737200, 'Raw straddle P\\&L (option + delta)'),
    ('option_pnl_cube', 'pnl', 17737200, 'Option-only P\\&L'),
    ('delta_pnl_cube', 'pnl', 17737200, 'Delta hedge P\\&L'),
    ('entry_cube', 'pnl', 17737200, 'Entry indicators (1 on entry day)'),
    ('expiry_cube', 'pnl', 17737200, 'Exit indicators (1 on expiry day)'),
    ('entry_weight_cube', 'pnl', 17737200, 'Stagger weights propagated for straddle life'),
    ('decay_cube', 'pnl', 17737200, 'Theta/decay P\\&L'),
    ('vega_cube', 'pnl', 17737200, 'Vega exposure'),
    ('vol_cube', 'pnl', 17737200, 'Implied volatility'),
    ('pnl_cube_slice', 'pnl', 10, 'Slice names (pnl\\_1 to pnl\\_10 = stagger weights)'),

    # Signal cubes
    ('signals', 'signal', 35474400, '3D cube: 9854×180×20 raw signal values'),
    ('signal_ranks', 'signal', 35474400, 'Cross-sectional signal ranks'),
    ('signals_signal', 'signal', 20, 'Signal names (pearson/tau/one/sign × rsi/calmar/etc.)'),
    ('residuals', 'signal', 7094880, '3D cube: 9854×180×4 factor-adjusted residuals'),
    ('residuals_residual', 'signal', 4, 'Residual types (pearson, tau, one, sign)'),

    # Reference cube
    ('reference', 'ref', 175598280, '3D cube: 9854×180×99 reference metrics'),
    ('reference_result', 'ref', 99, 'Reference slice names'),

    # Stats
    ('stats', 'stats', 29, 'Aggregated stats by class (mean, sharpe, hit, dd)'),
    ('signal_hedged_stats', 'stats', 580, 'Stats for hedged signal combos'),
    ('signal_norm_stats', 'stats', 580, 'Stats for normalized signal combos (unused)'),

    # Backtest detail
    ('backtest', 'bt', 6230449, 'Straddle-day level P\\&L with all fields'),
    ('backtest_pnl_ndx', 'bt', 6230954, 'Indexed P\\&L for fast lookup'),
    ('recent_entries', 'bt', 4294, 'Currently active straddles'),

    # Straddle files
    ('straddles1', 'strad', 192246, 'Straddle definitions (tickers, entry/expiry)'),
    ('straddles5', 'strad', 6404102, 'Straddle valuations over time'),
    ('straddle_entry_expiry', 'strad', 192246, 'Entry/expiry dates per straddle'),
    ('straddle_ndx', 'strad', 186685, 'Straddle indexing'),
    ('straddle_start_ndx', 'strad', 186692, 'Straddle start date indices'),
    ('straddle_tickers', 'strad', 192246, 'Ticker mapping for straddles'),
    ('straddle_biz', 'strad', 240084, 'Business day info (has\\_hedge, has\\_vol)'),
    ('straddle_days', 'strad', 240084, 'Calendar day breakdown'),

    # Price files
    ('prices', 'price', 8506227, 'Combined price data (ticker, date, field, value)'),
    ('bbg_prices', 'price', 7143932, 'Raw Bloomberg prices'),
    ('calc_prices', 'price', 1293084, 'Calculated/derived prices'),
    ('cv_prices', 'price', 69247, 'CV prices'),

    # Ticker files
    ('tickers', 'ticker', 5078, 'Master ticker list (source, ticker, field)'),
    ('hedge_tickers', 'ticker', 303912, 'Hedge ticker assignments'),
    ('vol_tickers', 'ticker', 240084, 'Volatility ticker assignments'),

    # Other
    ('cds_roll_dates', 'other', 54, 'CDS roll dates'),
    ('current_bbg_chain_data', 'other', 6985, 'Bloomberg futures chain data'),
    ('expiry_overrides', 'other', 6100, 'Manual expiry date overrides'),
]

# Category colors and labels
cat_colors = {
    'dim': ('blue!15', 'Dimension'),
    'config': ('orange!15', 'Config'),
    'cube': ('green!15', 'Strategy'),
    'pnl': ('red!15', 'P\\&L Cube'),
    'signal': ('purple!15', 'Signal'),
    'ref': ('cyan!15', 'Reference'),
    'stats': ('yellow!20', 'Stats'),
    'bt': ('brown!15', 'Backtest'),
    'strad': ('pink!15', 'Straddle'),
    'price': ('olive!15', 'Price'),
    'ticker': ('teal!15', 'Ticker'),
    'other': ('gray!15', 'Other'),
}

def file_contents_to_latex(data, colors):
    lines = []
    lines.append(r'\begin{longtable}{lrlp{7.5cm}}')
    lines.append(r'\caption{File Contents Reference} \\')
    lines.append(r'\toprule')
    lines.append(r'\textbf{File} & \textbf{Rows} & \textbf{Category} & \textbf{Description} \\')
    lines.append(r'\midrule')
    lines.append(r'\endfirsthead')
    lines.append(r'\toprule')
    lines.append(r'\textbf{File} & \textbf{Rows} & \textbf{Category} & \textbf{Description} \\')
    lines.append(r'\midrule')
    lines.append(r'\endhead')
    lines.append(r'\midrule')
    lines.append(r'\multicolumn{4}{r}{\textit{Continued...}} \\')
    lines.append(r'\endfoot')
    lines.append(r'\bottomrule')
    lines.append(r'\endlastfoot')

    for fname, cat, rows, desc in data:
        color, cat_label = colors[cat]
        fname_esc = fname.replace('_', r'\_')
        if rows >= 1000000:
            rows_str = f'{rows/1000000:.1f}M'
        elif rows >= 1000:
            rows_str = f'{rows/1000:.0f}K'
        else:
            rows_str = str(rows)
        lines.append(
            r'\rowcolor{' + color + '}' +
            f'{fname_esc} & {rows_str} & {cat_label} & {desc} ' + r'\\'
        )

    lines.append(r'\end{longtable}')
    return '\n'.join(lines)

file_contents_latex = file_contents_to_latex(file_contents_data, cat_colors)

# --- Generate Cubes Table ---
def cubes_to_latex(df):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Cube Files Summary}')
    lines.append(r'\label{tab:cubes}')
    lines.append(r'\begin{tabular}{lrr}')
    lines.append(r'\toprule')
    lines.append(r'Cube & Rows & Size (MB) \\')
    lines.append(r'\midrule')

    for _, row in df.iterrows():
        name = row['cube'].replace('_', r'\_')
        lines.append(f"{name} & {row['rows']:,} & {row['size_mb']:.1f} " + r'\\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

cubes_latex = cubes_to_latex(cubes_df)

# --- Generate Assets Table (3 columns) ---
def assets_to_latex(df):
    n = len(df)
    third = (n + 2) // 3

    left = df.iloc[:third].reset_index(drop=True)
    mid = df.iloc[third:2*third].reset_index(drop=True)
    right = df.iloc[2*third:].reset_index(drop=True)

    max_len = max(len(left), len(mid), len(right))

    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Assets in Universe}')
    lines.append(r'\label{tab:assets}')
    lines.append(r'\scriptsize')
    lines.append(r'\begin{tabular}{rl|rl|rl}')
    lines.append(r'\toprule')
    lines.append(r'ID & Asset & ID & Asset & ID & Asset \\')
    lines.append(r'\midrule')

    for i in range(max_len):
        cells = []
        for d in [left, mid, right]:
            if i < len(d):
                idx = d.loc[i, 'asset']
                name = d.loc[i, 'asset_string'].replace('_', r'\_')
                cells.extend([str(idx), name])
            else:
                cells.extend(['', ''])
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

assets_latex = assets_to_latex(asset_df)

# --- Generate Strategy Slices Table ---
def slices_to_latex(df, caption, label):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(f'\\caption{{{caption}}}')
    lines.append(f'\\label{{tab:{label}}}')
    lines.append(r'\begin{tabular}{rl}')
    lines.append(r'\toprule')
    lines.append(r'ID & Slice \\')
    lines.append(r'\midrule')

    for _, row in df.iterrows():
        idx = row.iloc[1]
        name = row.iloc[0].replace('_', r'\_')
        lines.append(f"{idx} & {name} " + r'\\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

strategy_slices_latex = slices_to_latex(strategy_result, 'Strategy Cube Slices', 'strategy_slices')
signals_latex = slices_to_latex(signals_signal, 'Signal Types', 'signals')
pnl_slices_latex = slices_to_latex(pnl_slice, 'PnL Cube Slices (Stagger Weights)', 'pnl_slices')

# --- Generate Reference Result Slices Table (3 columns) ---
def reference_slices_to_latex(df):
    n = len(df)
    third = (n + 2) // 3

    left = df.iloc[:third].reset_index(drop=True)
    mid = df.iloc[third:2*third].reset_index(drop=True)
    right = df.iloc[2*third:].reset_index(drop=True)

    max_len = max(len(left), len(mid), len(right))

    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Reference Cube Slices (' + str(n) + r' result types)}')
    lines.append(r'\label{tab:reference_slices}')
    lines.append(r'\scriptsize')
    lines.append(r'\begin{tabular}{rl|rl|rl}')
    lines.append(r'\toprule')
    lines.append(r'ID & Slice & ID & Slice & ID & Slice \\')
    lines.append(r'\midrule')

    for i in range(max_len):
        cells = []
        for d in [left, mid, right]:
            if i < len(d):
                idx = d.iloc[i, 1]  # result column
                name = d.iloc[i, 0].replace('_', r'\_')  # result_string column
                cells.extend([str(int(idx)), name])
            else:
                cells.extend(['', ''])
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

reference_slices_latex = reference_slices_to_latex(reference_result)

# --- Generate Stats Table ---
def stats_to_latex(df):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Backtest Statistics by Asset Class}')
    lines.append(r'\label{tab:stats}')
    lines.append(r'\small')

    cols = df.columns.tolist()
    lines.append(r'\begin{tabular}{l' + 'r' * (len(cols) - 1) + '}')
    lines.append(r'\toprule')
    header = ' & '.join([c.replace('_', r'\_') for c in cols])
    lines.append(header + r' \\')
    lines.append(r'\midrule')

    for _, row in df.iterrows():
        cells = []
        for i, c in enumerate(cols):
            val = row[c]
            if i == 0:
                cells.append(str(val).replace('_', r'\_'))
            elif pd.isna(val):
                cells.append('')
            else:
                cells.append(f'{val:.2f}')
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

stats_latex = stats_to_latex(stats_df)

# --- Generate Signal Hedged Stats Tables (4 signals per table, stacked) ---
signal_hedged_stats_df = pd.read_parquet(f"{RESULT_DIR}/signal_hedged_stats.parquet")

# Remove non-useful statistics
exclude_stats = ['MAD_nz_pnl', 'Q_nz_pnl', 'R_nz_pnl', 'S_nz_pnl', 'T_nz_pnl',
                 'sd_ratio_xy_nz', 'sd_ratio_q50xy_xy', 'hit_ratio_ratio_nz', 'winloss_ratio_nz',
                 'dudd_pnl', 'dudd_pve', 'dudd_nve', 'hit_pnl', 'hit_pve', 'hit_nve']
signal_hedged_stats_df = signal_hedged_stats_df[~signal_hedged_stats_df['name'].isin(exclude_stats)]

# Abbreviated column headers
col_abbrev = {'commodity': 'cmd', 'equity': 'eq', 'fx': 'fx', 'rates': 'rat', 'stonks': 'stk', 'all': 'all'}
cols = ['commodity', 'equity', 'fx', 'rates', 'stonks', 'all']

def format_val(val):
    if pd.isna(val):
        return ''
    elif abs(val) >= 100:
        return f'{val:.0f}'
    elif abs(val) >= 10:
        return f'{val:.1f}'
    else:
        return f'{val:.2f}'

def signal_octet_to_latex(df, signals):
    """Generate a table for up to 8 signals: pairs stacked vertically."""
    abbrevs = [col_abbrev[c] for c in cols]

    # Get dataframes for each signal
    dfs = [df[df['signal'] == sig].copy().reset_index(drop=True) for sig in signals]
    sigs_esc = [sig.replace('_', r'\_') for sig in signals]

    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\scriptsize')
    lines.append(r'\begin{tabular}{l|rrrrrr|rrrrrr}')
    lines.append(r'\toprule')

    # Asset class abbreviations at top (common for all pairs)
    header_abbrevs = 'Stat & ' + ' & '.join(abbrevs) + ' & ' + ' & '.join(abbrevs) + r' \\'
    lines.append(header_abbrevs)

    # Each pair: signal names as separator, then data
    n_pairs = (len(signals) + 1) // 2
    for p in range(n_pairs):
        idx1 = p * 2
        idx2 = p * 2 + 1

        lines.append(r'\midrule')

        # Signal names as separator row
        if idx2 < len(signals):
            sig_header = r' & \multicolumn{6}{c|}{' + sigs_esc[idx1] + r'} & \multicolumn{6}{c}{' + sigs_esc[idx2] + r'} \\'
        else:
            # Odd number of signals - only one in last pair
            sig_header = r' & \multicolumn{6}{c|}{' + sigs_esc[idx1] + r'} & \multicolumn{6}{c}{} \\'
        lines.append(sig_header)
        lines.append(r'\midrule')

        # Data rows
        for i in range(len(dfs[idx1])):
            stat_name_raw = dfs[idx1].iloc[i]['name']
            stat_name = stat_name_raw.replace('_', r'\_')
            is_sharpe = (stat_name_raw == 'sharpe_pnl')
            cells = [stat_name]
            for col in cols:
                val = format_val(dfs[idx1].iloc[i][col])
                if is_sharpe and col == 'all':
                    val = r'\textbf{' + val + '}'
                cells.append(val)
            if idx2 < len(signals):
                for col in cols:
                    val = format_val(dfs[idx2].iloc[i][col])
                    if is_sharpe and col == 'all':
                        val = r'\textbf{' + val + '}'
                    cells.append(val)
            else:
                cells.extend([''] * 6)
            lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

# Get unique signals in order
signal_names = signal_hedged_stats_df['signal'].unique().tolist()

# Generate tables with up to 8 signals per table (pairs stacked)
signal_hedged_tables_latex = []
for i in range(0, len(signal_names), 8):
    batch = signal_names[i:i+8]
    signal_hedged_tables_latex.append(signal_octet_to_latex(signal_hedged_stats_df, batch))

signal_hedged_all_latex = '\n\n'.join(signal_hedged_tables_latex)

# --- Reference P&L Stats (using hedged_01 and wpnl from reference cube) ---
# Load reference cube
ref_cube_df = pd.read_parquet(f"{RESULT_DIR}/reference.parquet")
ref_date_df = pd.read_parquet(f"{RESULT_DIR}/reference_date.parquet")
ref_asset_df = pd.read_parquet(f"{RESULT_DIR}/reference_asset.parquet")
ref_result_df = pd.read_parquet(f"{RESULT_DIR}/reference_result.parquet")

n_ref_dates = len(ref_date_df)
n_ref_assets = len(ref_asset_df)
n_ref_results = len(ref_result_df)
# Sort by (date, asset, result) before reshape - critical!
ref_cube_sorted = ref_cube_df.sort_values(['date', 'asset', 'result'])
ref_cube = ref_cube_sorted['value'].values.reshape((n_ref_dates, n_ref_assets, n_ref_results), order='C')

# Get slice indices
result_names = ref_result_df['result_string'].tolist()
hedged_01_idx = result_names.index('hedged_01')
wpnl_idx = result_names.index('wpnl')
asset_class_idx = result_names.index('asset_class')

# Extract slices
hedged_01_weights = ref_cube[:, :, hedged_01_idx]  # (dates, assets)
wpnl_from_ref = ref_cube[:, :, wpnl_idx]  # (dates, assets) - stored wpnl
asset_classes = ref_cube[0, :, asset_class_idx]  # asset class per asset (constant over time)

# Map asset class codes to names (0=commodity, 1=equity, 2=fx, 3=rates, 4=stonks)
class_map = {0: 'commodity', 1: 'equity', 2: 'fx', 3: 'rates', 4: 'stonks'}

# Daily wpnl (rowwise sum of wpnl from reference cube)
daily_wpnl_ref = wpnl_from_ref.sum(axis=1) * 10000  # to bps
daily_wpnl_ref_nz = daily_wpnl_ref[daily_wpnl_ref != 0]
ref_wpnl_sharpe_nz = (daily_wpnl_ref_nz.mean() / daily_wpnl_ref_nz.std() * np.sqrt(256)) if len(daily_wpnl_ref_nz) > 1 and daily_wpnl_ref_nz.std() > 0 else 0

# Long/short P&L from hedged_01 sign
long_mask = hedged_01_weights > 0
short_mask = hedged_01_weights < 0

# Compute long/short wpnl by asset class
ref_long_pnl_by_class = {}
ref_short_pnl_by_class = {}

for cls_code, cls_name in class_map.items():
    cls_mask = asset_classes == cls_code
    if cls_mask.sum() > 0:
        long_pnl_cls = (wpnl_from_ref[:, cls_mask] * long_mask[:, cls_mask]).sum(axis=1) * 10000
        short_pnl_cls = (wpnl_from_ref[:, cls_mask] * short_mask[:, cls_mask]).sum(axis=1) * 10000
        ref_long_pnl_by_class[cls_name] = pd.Series(long_pnl_cls)
        ref_short_pnl_by_class[cls_name] = pd.Series(short_pnl_cls)

# Total long/short
long_pnl_total = (wpnl_from_ref * long_mask).sum(axis=1) * 10000
short_pnl_total = (wpnl_from_ref * short_mask).sum(axis=1) * 10000
ref_long_pnl_by_class['all'] = pd.Series(long_pnl_total)
ref_short_pnl_by_class['all'] = pd.Series(short_pnl_total)

# Daily wpnl by asset class
ref_daily_pnl_by_class = {}
for cls_code, cls_name in class_map.items():
    cls_mask = asset_classes == cls_code
    if cls_mask.sum() > 0:
        ref_daily_pnl_by_class[cls_name] = pd.Series(wpnl_from_ref[:, cls_mask].sum(axis=1) * 10000)
ref_daily_pnl_by_class['all'] = pd.Series(wpnl_from_ref.sum(axis=1) * 10000)

def compute_max_drawdown_ref(pnl_series):
    cumulative = pnl_series.cumsum()
    running_max = cumulative.cummax()
    drawdown = cumulative - running_max
    return drawdown.min()

# Compute stats (matching P&L Stats format)
ref_stats_rows = []
class_order = sorted([c for c in ref_daily_pnl_by_class.keys() if c != 'all']) + ['all']

for cls in class_order:
    pnl = ref_daily_pnl_by_class[cls]
    pnl_nz = pnl[pnl != 0]

    mean_pnl = pnl_nz.mean() if len(pnl_nz) > 0 else 0
    mean_abs_pnl = pnl_nz.abs().mean() if len(pnl_nz) > 0 else 0
    sharpe = (pnl_nz.mean() / pnl_nz.std()) * np.sqrt(256) if len(pnl_nz) > 1 and pnl_nz.std() > 0 else 0
    hit_ratio = (pnl_nz > 0).sum() / len(pnl_nz) * 100 if len(pnl_nz) > 0 else 0
    max_dd = compute_max_drawdown_ref(pnl_nz)

    # Long/short stats
    long_pnl_cls = ref_long_pnl_by_class.get(cls, pd.Series([0]))
    short_pnl_cls = ref_short_pnl_by_class.get(cls, pd.Series([0]))

    long_nz = long_pnl_cls[long_pnl_cls != 0]
    short_nz = short_pnl_cls[short_pnl_cls != 0]

    mean_pve_pnl = long_nz.mean() if len(long_nz) > 0 else 0
    mean_nve_pnl = short_nz.mean() if len(short_nz) > 0 else 0
    mean_abs_pve_pnl = long_nz.abs().mean() if len(long_nz) > 0 else 0
    mean_abs_nve_pnl = short_nz.abs().mean() if len(short_nz) > 0 else 0

    pve_sharpe = (long_nz.mean() / long_nz.std()) * np.sqrt(256) if len(long_nz) > 1 and long_nz.std() > 0 else 0
    nve_sharpe = (short_nz.mean() / short_nz.std()) * np.sqrt(256) if len(short_nz) > 1 and short_nz.std() > 0 else 0

    pve_hit_ratio = (long_nz > 0).sum() / len(long_nz) * 100 if len(long_nz) > 0 else 0
    nve_hit_ratio = (short_nz > 0).sum() / len(short_nz) * 100 if len(short_nz) > 0 else 0

    # CorLS: correlation of long to short P&L (exclude rows where either is zero)
    both_nz_mask = (long_pnl_cls != 0) & (short_pnl_cls != 0)
    long_both_nz = long_pnl_cls[both_nz_mask]
    short_both_nz = short_pnl_cls[both_nz_mask]
    if len(long_both_nz) > 1 and len(short_both_nz) > 1:
        cor_ls = long_both_nz.corr(short_both_nz)
    else:
        cor_ls = 0

    ref_stats_rows.append({
        'class': cls,
        'mean_pnl': mean_pnl,
        'mean_pve_pnl': mean_pve_pnl,
        'mean_nve_pnl': mean_nve_pnl,
        'mean_abs_pnl': mean_abs_pnl,
        'mean_abs_pve_pnl': mean_abs_pve_pnl,
        'mean_abs_nve_pnl': mean_abs_nve_pnl,
        'sharpe': sharpe,
        'pve_sharpe': pve_sharpe,
        'nve_sharpe': nve_sharpe,
        'hit_ratio': hit_ratio,
        'pve_hit_ratio': pve_hit_ratio,
        'nve_hit_ratio': nve_hit_ratio,
        'drawdown': max_dd,
        'cor_ls': cor_ls
    })

ref_stats_df = pd.DataFrame(ref_stats_rows)
ref_stats_df.set_index('class', inplace=True)
ref_stats_t = ref_stats_df.T

# Generate LaTeX table (same format as P&L Stats)
def ref_stats_to_latex(df):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Reference P\&L Statistics (hedged\_01 weights, daily bps)}')
    lines.append(r'\label{tab:ref_pnl_stats}')
    lines.append(r'\small')

    n_classes = len(df.columns) - 1
    lines.append(r'\begin{tabular}{l' + 'r' * n_classes + '|r}')
    lines.append(r'\toprule')
    lines.append('Statistic & ' + ' & '.join(df.columns) + r' \\')
    lines.append(r'\midrule')

    row_labels = {
        'mean_pnl': 'Mean P\\&L',
        'mean_pve_pnl': 'Mean(pve P\\&L)',
        'mean_nve_pnl': 'Mean(nve P\\&L)',
        'mean_abs_pnl': 'Mean $|$P\\&L$|$',
        'mean_abs_pve_pnl': 'Mean $|$pve P\\&L$|$',
        'mean_abs_nve_pnl': 'Mean $|$nve P\\&L$|$',
        'sharpe': 'Sharpe (ann.)',
        'pve_sharpe': 'pve Sharpe',
        'nve_sharpe': 'nve Sharpe',
        'hit_ratio': 'Hit Ratio (\\%)',
        'pve_hit_ratio': 'pve Hit (\\%)',
        'nve_hit_ratio': 'nve Hit (\\%)',
        'drawdown': 'Max Drawdown',
        'cor_ls': 'CorLS'
    }

    row_order = [
        'mean_pnl', 'mean_pve_pnl', 'mean_nve_pnl',
        'mean_abs_pnl', 'mean_abs_pve_pnl', 'mean_abs_nve_pnl',
        'sharpe', 'pve_sharpe', 'nve_sharpe',
        'hit_ratio', 'pve_hit_ratio', 'nve_hit_ratio',
        'drawdown', 'cor_ls'
    ]

    for stat in row_order:
        if stat not in df.index:
            continue
        cells = [row_labels.get(stat, stat)]
        for cls in df.columns:
            val = df.loc[stat, cls]
            if stat in ('mean_pnl', 'mean_pve_pnl', 'mean_nve_pnl',
                        'mean_abs_pnl', 'mean_abs_pve_pnl', 'mean_abs_nve_pnl'):
                cells.append(f'{val:.2f}')
            elif stat in ('sharpe', 'pve_sharpe', 'nve_sharpe'):
                formatted = f'{val:.2f}'
                if stat == 'sharpe' and cls == 'all':
                    formatted = r'\textbf{' + formatted + '}'
                cells.append(formatted)
            elif stat == 'cor_ls':
                cells.append(f'{val * 100:.2f}')
            elif stat in ('hit_ratio', 'pve_hit_ratio', 'nve_hit_ratio'):
                cells.append(f'{val:.1f}')
            else:
                cells.append(f'{int(round(val)):,}')
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

ref_stats_latex = ref_stats_to_latex(ref_stats_t)

# Bullet point for wpnl sharpe_nz
ref_wpnl_bullet = f"\\textbf{{Stored wpnl Sharpe (nz):}} {ref_wpnl_sharpe_nz:.2f}"

# --- Load and Plot wpnl from strategy cube ---
# Load strategy cube data for wpnl slice
strategy_data = pd.read_parquet(f"{RESULT_DIR}/strategy.parquet")
strategy_date = pd.read_parquet(f"{RESULT_DIR}/strategy_date.parquet")
strategy_asset = pd.read_parquet(f"{RESULT_DIR}/strategy_asset.parquet")

# Get wpnl slice (result=22)
wpnl_data = strategy_data[strategy_data['result'] == 22].copy()

# Merge with date strings
wpnl_data = wpnl_data.merge(strategy_date, on='date')
wpnl_data['date_dt'] = pd.to_datetime(wpnl_data['date_string'])

# Daily total P&L
daily_pnl = wpnl_data.groupby('date_dt')['value'].sum() * 10000  # to bps
daily_pnl = daily_pnl[daily_pnl != 0]

# Cumulative P&L (will be plotted after long/short are computed)
cumulative_pnl = daily_pnl.cumsum()

# Monthly P&L distribution
monthly_pnl = daily_pnl.resample('ME').sum()
monthly_pnl = monthly_pnl[monthly_pnl != 0]

fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(monthly_pnl.values, bins=40, edgecolor='black', alpha=0.7)
ax.axvline(0, color='red', linestyle='--', linewidth=2)
ax.axvline(monthly_pnl.mean(), color='green', linestyle='--', linewidth=2,
           label=f'Mean: {monthly_pnl.mean():.0f} bps')
ax.set_xlabel('Monthly P&L (bps)')
ax.set_ylabel('Frequency')
ax.set_title('Monthly P&L Distribution')
ax.legend()
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_monthly_hist.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# (Cumulative P&L and Rolling Vol plots are generated after class_daily_pnl is computed)

# --- P&L Stats by Class ---
# Compute stats from daily wpnl by asset class

# Get hedged weights (result=21) to determine asset class allocation
hedged_data = strategy_data[strategy_data['result'] == 21].copy()
hedged_data = hedged_data.merge(strategy_date, on='date')
hedged_data = hedged_data.merge(strategy_asset, on='asset')
hedged_data['date_dt'] = pd.to_datetime(hedged_data['date_string'])

# Load asset class mapping from strategy cube (result=1 is asset_class)
class_data = strategy_data[strategy_data['result'] == 1].copy()
class_data = class_data.merge(strategy_asset, on='asset')
# Get first non-zero class for each asset
asset_class_map = class_data.groupby('asset_string')['value'].first().to_dict()

# Load class names from group_names.parquet
group_names_df = pd.read_parquet(f"{RESULT_DIR}/group_names.parquet")
class_names = dict(zip(group_names_df['value'].astype(int), group_names_df['name']))
class_name_list = group_names_df['name'].tolist()  # ordered list of class names

# Get wpnl by asset
wpnl_by_asset = wpnl_data.pivot(index='date_dt', columns='asset', values='value') * 10000

# Map asset indices to class names
asset_to_class = {}
for _, row in strategy_asset.iterrows():
    asset_idx = row['asset']
    asset_name = row['asset_string']
    class_val = asset_class_map.get(asset_name, 0)
    asset_to_class[asset_idx] = class_names.get(int(class_val), 'other')

# Aggregate by class
class_daily_pnl = {}
for cls in class_names.values():
    cls_assets = [a for a, c in asset_to_class.items() if c == cls]
    if cls_assets:
        cls_cols = [a for a in cls_assets if a in wpnl_by_asset.columns]
        if cls_cols:
            class_daily_pnl[cls] = wpnl_by_asset[cls_cols].sum(axis=1)

class_daily_pnl['Total'] = wpnl_by_asset.sum(axis=1)

# Cumulative P&L by asset class
fig, ax = plt.subplots(figsize=(12, 6))
colors_map = {'commodity': 'brown', 'equity': 'blue', 'fx': 'green',
              'rates': 'purple', 'stonks': 'orange', 'Total': 'black'}
for cls in class_name_list + ['Total']:
    if cls in class_daily_pnl:
        cumul = class_daily_pnl[cls].cumsum()
        lw = 2.5 if cls == 'Total' else 1.2
        ax.plot(cumul.index, cumul.values, label=cls, color=colors_map.get(cls, 'gray'), linewidth=lw)

ax.axhline(0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('Date')
ax.set_ylabel('Cumulative P&L (bps)')
ax.set_title('Cumulative P&L by Asset Class')
ax.legend(loc='upper left')
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_cumulative_by_class.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# Rolling volatility by asset class (EMA of abs(pnl) with 364/365 decay)
alpha = 1 - 364/365  # EMA decay factor

fig, ax = plt.subplots(figsize=(12, 6))
for cls in class_name_list + ['Total']:
    if cls in class_daily_pnl:
        abs_pnl = class_daily_pnl[cls].abs()
        rolling_vol = abs_pnl.ewm(alpha=alpha, adjust=False).mean()
        lw = 2.5 if cls == 'Total' else 1.2
        ax.plot(rolling_vol.index, rolling_vol.values, label=cls,
                color=colors_map.get(cls, 'gray'), linewidth=lw)

ax.set_xlabel('Date')
ax.set_ylabel('Rolling Volatility (bps)')
ax.set_title('Rolling Volatility by Asset Class (EMA of |P&L|, 364/365 decay)')
ax.legend(loc='upper left')
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_rolling_vol_by_class.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# Realized correlation: Long vs Short P&L (using cubes with inception weights)
# Load pnl_cube as 3D array
pnl_cube_3d, pnl_cube_date_df, pnl_cube_asset_df, _ = load_cube_as_3d(RESULT_DIR, 'pnl_cube')

# Load entry_cube (trigger points for inception weighting)
entry_cube_3d, _, _, _ = load_cube_as_3d(RESULT_DIR, 'entry_cube')

# Load hedged weights from strategy cube
strategy_result_df = pd.read_parquet(f"{RESULT_DIR}/strategy_result.parquet")
strategy_cube_df = pd.read_parquet(f"{RESULT_DIR}/strategy.parquet")
strategy_date_df = pd.read_parquet(f"{RESULT_DIR}/strategy_date.parquet")
strategy_asset_df = pd.read_parquet(f"{RESULT_DIR}/strategy_asset.parquet")

n_strat_dates = len(strategy_date_df)
n_strat_assets = len(strategy_asset_df)
n_strat_results = len(strategy_result_df)
strategy_cube = strategy_cube_df['value'].values.reshape((n_strat_dates, n_strat_assets, n_strat_results), order='C')

# Get hedged slice (daily hedged weights with sign)
slice_names = strategy_result_df['result_string'].tolist()
hedged_idx = slice_names.index('hedged')
hedged_mat = strategy_cube[:, :, hedged_idx]

# Use cube_rwlf to propagate inception hedged weights: at each entry point, grab
# the hedged weight and carry forward for the life of the straddle
inception_hedged = cube_rwlf(hedged_mat.astype(np.float64), entry_cube_3d.astype(np.float64))

# Weighted P&L = pnl * inception_hedged
wpnl_cube_3d = pnl_cube_3d * inception_hedged

# Get stored wpnl from strategy cube for error comparison
wpnl_idx = slice_names.index('wpnl')
wpnl_stored = strategy_cube[:, :, wpnl_idx]  # shape: (date, asset)

# Computed wpnl = sum across slices
wpnl_computed = wpnl_cube_3d.sum(axis=2)  # shape: (date, asset)

# Compute absolute error per (date, asset)
wpnl_error = np.abs(wpnl_computed - wpnl_stored)

# Sum error across assets per date
wpnl_error_daily = wpnl_error.sum(axis=1) * 10000  # to bps

# Create error series with date index
strat_dates = pd.to_datetime(strategy_date_df['date_string'].values)
wpnl_error_series = pd.Series(wpnl_error_daily, index=strat_dates)

# Split by sign of inception weight (long = positive, short = negative)
long_mask = inception_hedged > 0
short_mask = inception_hedged < 0

# Sum weighted P&L by long/short classification across assets and slices
long_pnl_daily = (wpnl_cube_3d * long_mask).sum(axis=(1, 2)) * 10000  # to bps
short_pnl_daily = (wpnl_cube_3d * short_mask).sum(axis=(1, 2)) * 10000  # to bps

# Create date index
pnl_dates = pd.to_datetime(pnl_cube_date_df['date_string'].values)
long_pnl = pd.Series(long_pnl_daily, index=pnl_dates)
short_pnl = pd.Series(short_pnl_daily, index=pnl_dates)

# Plot cumulative P&L with long/short breakdown
# Note: "long" has negative weights (short straddles), "short" has positive weights
cumul_total = (long_pnl + short_pnl).cumsum()
cumul_long = long_pnl.cumsum()    # negative weights = short straddles
cumul_short = short_pnl.cumsum()  # positive weights = long straddles

# Compute drawdown
drawdown_series = cumul_total - cumul_total.cummax()

# Cumulative P&L with drawdown shading (2-panel)
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)

ax1.plot(cumul_total.index, cumul_total.values, linewidth=2, color='black', label='Total')
ax1.plot(cumul_long.index, cumul_long.values, linewidth=1.2, color='red', label='Long (nve weights)')
ax1.plot(cumul_short.index, cumul_short.values, linewidth=1.2, color='green', label='Short (pve weights)')
ax1.fill_between(cumul_total.index, cumul_total.values, cumul_total.cummax().values, color='red', alpha=0.15)
ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)
ax1.set_ylabel('Cumulative P&L (bps)')
ax1.set_title('Cumulative Strategy P&L')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.fill_between(drawdown_series.index, drawdown_series.values, 0, color='red', alpha=0.4)
ax2.plot(drawdown_series.index, drawdown_series.values, linewidth=0.8, color='darkred')
ax2.set_ylabel('Drawdown (bps)')
ax2.set_xlabel('Date')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_cumulative_pnl.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# Standalone drawdown chart by asset class
fig, ax = plt.subplots(figsize=(12, 5))
colors_map_dd = {'commodity': 'brown', 'equity': 'blue', 'fx': 'green', 'rates': 'orange', 'stonks': 'purple'}
for cls in ['commodity', 'equity', 'fx', 'rates', 'stonks']:
    if cls in class_daily_pnl:
        cls_cumul = class_daily_pnl[cls].cumsum()
        cls_dd = cls_cumul - cls_cumul.cummax()
        ax.plot(cls_dd.index, cls_dd.values, linewidth=1, color=colors_map_dd.get(cls, 'gray'), label=cls, alpha=0.7)
# Total drawdown
ax.plot(drawdown_series.index, drawdown_series.values, linewidth=2, color='black', label='Total')
ax.set_xlabel('Date')
ax.set_ylabel('Drawdown (bps)')
ax.set_title('Drawdown by Asset Class')
ax.legend(loc='lower left')
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_drawdown_by_class.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# Remove days where both are zero
valid_days = (long_pnl != 0) | (short_pnl != 0)
long_pnl = long_pnl[valid_days]
short_pnl = short_pnl[valid_days]

# Compute rolling correlation (252-day window)
rolling_corr = long_pnl.rolling(window=252, min_periods=60).corr(short_pnl)

# Plot
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(rolling_corr.index, rolling_corr.values, linewidth=1, color='blue')
ax.axhline(0, color='gray', linestyle='--', alpha=0.5)
ax.axhline(rolling_corr.mean(), color='red', linestyle='--', alpha=0.7,
           label=f'Mean: {rolling_corr.mean():.2f}')
ax.set_xlabel('Date')
ax.set_ylabel('Correlation')
ax.set_title('Realized Correlation: Long vs Short P&L (252-day rolling, inception weights)')
ax.set_ylim(-1, 1)
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_long_short_corr.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# Realized correlation by asset class
# Need to map asset indices to classes
# Build asset index to class mapping
asset_idx_to_class = {}
for asset_idx in range(pnl_cube_3d.shape[1]):
    # asset indices are 1-based in the data, but 0-based in the cube
    asset_name = pnl_cube_asset_df.iloc[asset_idx]['asset_string']
    class_val = asset_class_map.get(asset_name, 0)
    asset_idx_to_class[asset_idx] = class_names.get(int(class_val), 'other')

fig, ax = plt.subplots(figsize=(12, 6))

for cls in class_name_list:
    # Get asset indices in this class
    cls_asset_indices = [idx for idx, c in asset_idx_to_class.items() if c == cls]

    if len(cls_asset_indices) > 0:
        # Slice the cubes for this class
        wpnl_cls = wpnl_cube_3d[:, cls_asset_indices, :]
        inception_hedged_cls = inception_hedged[:, cls_asset_indices, :]

        long_mask_cls = inception_hedged_cls > 0
        short_mask_cls = inception_hedged_cls < 0

        long_pnl_cls = (wpnl_cls * long_mask_cls).sum(axis=(1, 2)) * 10000
        short_pnl_cls = (wpnl_cls * short_mask_cls).sum(axis=(1, 2)) * 10000

        long_pnl_cls_series = pd.Series(long_pnl_cls, index=pnl_dates)
        short_pnl_cls_series = pd.Series(short_pnl_cls, index=pnl_dates)

        # Remove days where both are zero
        valid = (long_pnl_cls_series != 0) | (short_pnl_cls_series != 0)
        long_pnl_cls_series = long_pnl_cls_series[valid]
        short_pnl_cls_series = short_pnl_cls_series[valid]

        # Rolling correlation
        if len(long_pnl_cls_series) > 60:
            rolling_corr_cls = long_pnl_cls_series.rolling(window=252, min_periods=60).corr(short_pnl_cls_series)
            ax.plot(rolling_corr_cls.index, rolling_corr_cls.values,
                    label=cls, color=colors_map.get(cls, 'gray'), linewidth=1.2, alpha=0.8)

ax.axhline(0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('Date')
ax.set_ylabel('Correlation')
ax.set_title('Realized Correlation: Long vs Short P&L by Asset Class (252-day rolling, inception weights)')
ax.set_ylim(-1, 1)
ax.legend(loc='lower left')
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_long_short_corr_by_class.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# --- Cross-Asset-Class Correlation Matrix ---
class_corr_names = sorted([c for c in class_daily_pnl.keys() if c != 'Total'])
corr_df = pd.DataFrame({cls: class_daily_pnl[cls] for cls in class_corr_names})
# Only use days where at least one class is non-zero
corr_df = corr_df[(corr_df != 0).any(axis=1)]
cross_class_corr = corr_df.corr()

def cross_class_corr_to_latex(corr_mat):
    classes = corr_mat.columns.tolist()
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Cross-Asset-Class P\&L Correlation}')
    lines.append(r'\label{tab:cross_class_corr}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{l|' + 'r' * len(classes) + '}')
    lines.append(r'\toprule')
    lines.append(' & ' + ' & '.join(classes) + r' \\')
    lines.append(r'\midrule')
    for cls in classes:
        cells = [cls]
        for cls2 in classes:
            val = corr_mat.loc[cls, cls2]
            if cls == cls2:
                cells.append('--')
            else:
                cells.append(f'{val:.2f}')
        lines.append(' & '.join(cells) + r' \\')
    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

cross_class_corr_latex = cross_class_corr_to_latex(cross_class_corr)

def compute_max_drawdown(pnl_series):
    cumulative = pnl_series.cumsum()
    running_max = cumulative.cummax()
    drawdown = cumulative - running_max
    return drawdown.min()

# Compute long/short P&L per class for stats
class_long_pnl = {}
class_short_pnl = {}

for cls in class_name_list:
    cls_asset_indices = [idx for idx, c in asset_idx_to_class.items() if c == cls]
    if len(cls_asset_indices) > 0:
        wpnl_cls = wpnl_cube_3d[:, cls_asset_indices, :]
        inception_hedged_cls = inception_hedged[:, cls_asset_indices, :]

        long_mask_cls = inception_hedged_cls > 0
        short_mask_cls = inception_hedged_cls < 0

        long_pnl_cls = (wpnl_cls * long_mask_cls).sum(axis=(1, 2)) * 10000
        short_pnl_cls = (wpnl_cls * short_mask_cls).sum(axis=(1, 2)) * 10000

        class_long_pnl[cls] = pd.Series(long_pnl_cls, index=pnl_dates)
        class_short_pnl[cls] = pd.Series(short_pnl_cls, index=pnl_dates)

# Total long/short (already computed)
class_long_pnl['Total'] = long_pnl
class_short_pnl['Total'] = short_pnl

# Compute stats
pnl_stats_rows = []
class_order = sorted([c for c in class_daily_pnl.keys() if c != 'Total']) + ['Total']

for cls in class_order:
    pnl = class_daily_pnl[cls]
    pnl_nz = pnl[pnl != 0]

    mean_pnl = pnl_nz.mean() if len(pnl_nz) > 0 else 0
    mean_abs_pnl = pnl_nz.abs().mean() if len(pnl_nz) > 0 else 0
    sharpe = (pnl_nz.mean() / pnl_nz.std()) * np.sqrt(256) if len(pnl_nz) > 1 and pnl_nz.std() > 0 else 0
    hit_ratio = (pnl_nz > 0).sum() / len(pnl_nz) * 100 if len(pnl_nz) > 0 else 0
    max_dd = compute_max_drawdown(pnl_nz)

    # Long/short stats
    long_pnl_cls = class_long_pnl.get(cls, pd.Series([0]))
    short_pnl_cls = class_short_pnl.get(cls, pd.Series([0]))

    long_nz = long_pnl_cls[long_pnl_cls != 0]
    short_nz = short_pnl_cls[short_pnl_cls != 0]

    mean_pve_pnl = long_nz.mean() if len(long_nz) > 0 else 0
    mean_nve_pnl = short_nz.mean() if len(short_nz) > 0 else 0
    mean_abs_pve_pnl = long_nz.abs().mean() if len(long_nz) > 0 else 0
    mean_abs_nve_pnl = short_nz.abs().mean() if len(short_nz) > 0 else 0

    pve_sharpe = (long_nz.mean() / long_nz.std()) * np.sqrt(256) if len(long_nz) > 1 and long_nz.std() > 0 else 0
    nve_sharpe = (short_nz.mean() / short_nz.std()) * np.sqrt(256) if len(short_nz) > 1 and short_nz.std() > 0 else 0

    pve_hit_ratio = (long_nz > 0).sum() / len(long_nz) * 100 if len(long_nz) > 0 else 0
    nve_hit_ratio = (short_nz > 0).sum() / len(short_nz) * 100 if len(short_nz) > 0 else 0

    # Correlation of long to short: exclude rows where either is zero
    both_nz_mask = (long_pnl_cls != 0) & (short_pnl_cls != 0)
    long_both_nz = long_pnl_cls[both_nz_mask]
    short_both_nz = short_pnl_cls[both_nz_mask]
    if len(long_both_nz) > 1 and len(short_both_nz) > 1:
        cor_ls = long_both_nz.corr(short_both_nz)
    else:
        cor_ls = 0

    pnl_stats_rows.append({
        'class': cls,
        'mean_pnl': mean_pnl,
        'mean_pve_pnl': mean_pve_pnl,
        'mean_nve_pnl': mean_nve_pnl,
        'mean_abs_pnl': mean_abs_pnl,
        'mean_abs_pve_pnl': mean_abs_pve_pnl,
        'mean_abs_nve_pnl': mean_abs_nve_pnl,
        'sharpe': sharpe,
        'pve_sharpe': pve_sharpe,
        'nve_sharpe': nve_sharpe,
        'hit_ratio': hit_ratio,
        'pve_hit_ratio': pve_hit_ratio,
        'nve_hit_ratio': nve_hit_ratio,
        'drawdown': max_dd,
        'cor_ls': cor_ls
    })

pnl_stats_df = pd.DataFrame(pnl_stats_rows)
pnl_stats_df.set_index('class', inplace=True)
pnl_stats_t = pnl_stats_df.T

# Generate P&L Stats LaTeX
def pnl_stats_to_latex(df):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{P\&L Statistics by Asset Class (daily, bps)}')
    lines.append(r'\label{tab:pnl_stats}')
    lines.append(r'\small')

    n_classes = len(df.columns) - 1
    lines.append(r'\begin{tabular}{l' + 'r' * n_classes + '|r}')
    lines.append(r'\toprule')
    lines.append('Statistic & ' + ' & '.join(df.columns) + r' \\')
    lines.append(r'\midrule')

    row_labels = {
        'mean_pnl': 'Mean P\\&L',
        'mean_pve_pnl': 'Mean(pve P\\&L)',
        'mean_nve_pnl': 'Mean(nve P\\&L)',
        'mean_abs_pnl': 'Mean $|$P\\&L$|$',
        'mean_abs_pve_pnl': 'Mean $|$pve P\\&L$|$',
        'mean_abs_nve_pnl': 'Mean $|$nve P\\&L$|$',
        'sharpe': 'Sharpe (ann.)',
        'pve_sharpe': 'pve Sharpe',
        'nve_sharpe': 'nve Sharpe',
        'hit_ratio': 'Hit Ratio (\\%)',
        'pve_hit_ratio': 'pve Hit (\\%)',
        'nve_hit_ratio': 'nve Hit (\\%)',
        'drawdown': 'Max Drawdown',
        'cor_ls': 'CorLS'
    }

    # Define row order
    row_order = [
        'mean_pnl', 'mean_pve_pnl', 'mean_nve_pnl',
        'mean_abs_pnl', 'mean_abs_pve_pnl', 'mean_abs_nve_pnl',
        'sharpe', 'pve_sharpe', 'nve_sharpe',
        'hit_ratio', 'pve_hit_ratio', 'nve_hit_ratio',
        'drawdown', 'cor_ls'
    ]

    for stat in row_order:
        if stat not in df.index:
            continue
        cells = [row_labels.get(stat, stat)]
        for cls in df.columns:
            val = df.loc[stat, cls]
            if stat in ('mean_pnl', 'mean_pve_pnl', 'mean_nve_pnl',
                        'mean_abs_pnl', 'mean_abs_pve_pnl', 'mean_abs_nve_pnl'):
                cells.append(f'{val:.2f}')
            elif stat in ('sharpe', 'pve_sharpe', 'nve_sharpe'):
                cells.append(f'{val:.2f}')
            elif stat == 'cor_ls':
                cells.append(f'{val * 100:.2f}')
            elif stat in ('hit_ratio', 'pve_hit_ratio', 'nve_hit_ratio'):
                cells.append(f'{val:.1f}')
            else:
                cells.append(f'{int(round(val)):,}')
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

pnl_stats_latex = pnl_stats_to_latex(pnl_stats_t)

# --- Year-over-Year Performance Table ---
annual_pnl = daily_pnl.groupby(daily_pnl.index.year)
annual_rows = []
for year, group in annual_pnl:
    nz = group[group != 0]
    if len(nz) < 10:
        continue
    ann_sharpe = (nz.mean() / nz.std()) * np.sqrt(256) if nz.std() > 0 else 0
    ann_mean = nz.mean()
    ann_hit = (nz > 0).sum() / len(nz) * 100
    ann_cumsum = nz.cumsum()
    ann_dd = (ann_cumsum.cummax() - ann_cumsum).max()
    ann_total = nz.sum()
    annual_rows.append({
        'year': year, 'sharpe': ann_sharpe, 'mean_pnl': ann_mean,
        'hit_ratio': ann_hit, 'drawdown': -ann_dd, 'total_pnl': ann_total
    })
annual_df = pd.DataFrame(annual_rows)

def annual_to_latex(df):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Year-over-Year Performance (daily bps)}')
    lines.append(r'\label{tab:annual_perf}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{l|rrrrr}')
    lines.append(r'\toprule')
    lines.append(r'Year & Sharpe & Mean P\&L & Hit (\%) & Max DD & Total P\&L \\')
    lines.append(r'\midrule')
    for _, row in df.iterrows():
        cells = [
            str(int(row['year'])),
            f"{row['sharpe']:.2f}",
            f"{row['mean_pnl']:.2f}",
            f"{row['hit_ratio']:.1f}",
            f"{int(round(row['drawdown'])):,}",
            f"{int(round(row['total_pnl'])):,}"
        ]
        lines.append(' & '.join(cells) + r' \\')
    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

annual_perf_latex = annual_to_latex(annual_df)

# --- Signal Quality Ranking ---
# Extract sharpe_pnl for 'all' column from signal_hedged_stats_df and rank
signal_sharpes = signal_hedged_stats_df[
    (signal_hedged_stats_df['name'] == 'sharpe_pnl')
][['signal', 'all']].copy()
signal_sharpes.columns = ['signal', 'sharpe_all']
signal_sharpes = signal_sharpes.sort_values('sharpe_all', ascending=False).reset_index(drop=True)

# Also get sharpe by asset class
signal_rank_cols = ['commodity', 'equity', 'fx', 'rates', 'stonks', 'all']
signal_rank_rows = []
for _, row in signal_sharpes.iterrows():
    sig = row['signal']
    sig_data = signal_hedged_stats_df[
        (signal_hedged_stats_df['signal'] == sig) & (signal_hedged_stats_df['name'] == 'sharpe_pnl')
    ]
    if len(sig_data) > 0:
        r = {'signal': sig}
        for col in signal_rank_cols:
            r[col] = sig_data.iloc[0][col]
        signal_rank_rows.append(r)
signal_rank_df = pd.DataFrame(signal_rank_rows)

def signal_rank_to_latex(df):
    cols = ['commodity', 'equity', 'fx', 'rates', 'stonks', 'all']
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Signal Quality Ranking by Sharpe (hedged, all classes)}')
    lines.append(r'\label{tab:signal_ranking}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{l|rrrrr|r}')
    lines.append(r'\toprule')
    lines.append(r'Signal & Cmdty & Eqty & FX & Rates & Stnks & All \\')
    lines.append(r'\midrule')
    for _, row in df.iterrows():
        sig_esc = row['signal'].replace('_', r'\_')
        cells = [sig_esc]
        for col in cols:
            val = row[col]
            formatted = f'{val:.2f}'
            if col == 'all':
                formatted = r'\textbf{' + formatted + '}'
            cells.append(formatted)
        lines.append(' & '.join(cells) + r' \\')
    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

signal_ranking_latex = signal_rank_to_latex(signal_rank_df)

# --- Tail Risk Metrics ---
# VaR and CVaR at 95% and 99%
def compute_tail_risk(pnl_series):
    nz = pnl_series[pnl_series != 0]
    if len(nz) < 10:
        return {}
    var95 = np.percentile(nz, 5)
    var99 = np.percentile(nz, 1)
    cvar95 = nz[nz <= var95].mean()
    cvar99 = nz[nz <= var99].mean()
    return {'var95': var95, 'var99': var99, 'cvar95': cvar95, 'cvar99': cvar99}

tail_risk_rows = []
class_order_tr = sorted([c for c in class_daily_pnl.keys() if c != 'Total']) + ['Total']
for cls in class_order_tr:
    tr = compute_tail_risk(class_daily_pnl[cls])
    tr['class'] = cls
    tail_risk_rows.append(tr)
tail_risk_df = pd.DataFrame(tail_risk_rows)

# Monthly tail risk
monthly_total_pnl = daily_pnl.resample('ME').sum()
monthly_total_nz = monthly_total_pnl[monthly_total_pnl != 0]
monthly_tr = compute_tail_risk(monthly_total_nz)

def tail_risk_to_latex(df, monthly_tr):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Tail Risk Metrics (bps)}')
    lines.append(r'\label{tab:tail_risk}')
    lines.append(r'\small')
    n_cls = len(df) - 1
    lines.append(r'\begin{tabular}{l|' + 'r' * n_cls + '|r}')
    lines.append(r'\toprule')
    cls_names = df['class'].tolist()
    lines.append('Metric & ' + ' & '.join(cls_names) + r' \\')
    lines.append(r'\midrule')

    for metric, label in [('var95', 'Daily VaR (5\\%)'), ('cvar95', 'Daily CVaR (5\\%)'),
                           ('var99', 'Daily VaR (1\\%)'), ('cvar99', 'Daily CVaR (1\\%)')]:
        cells = [label]
        for _, row in df.iterrows():
            cells.append(f'{row[metric]:.1f}')
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\midrule')
    # Monthly tail risk (only for Total)
    for metric, label in [('var95', 'Monthly VaR (5\\%)'), ('cvar95', 'Monthly CVaR (5\\%)')]:
        cells = [label]
        for i in range(len(df)):
            if df.iloc[i]['class'] == 'Total':
                cells.append(f'{monthly_tr.get(metric, 0):.0f}')
            else:
                cells.append('--')
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

tail_risk_latex = tail_risk_to_latex(tail_risk_df, monthly_tr)

# --- P&L Component Decomposition ---
# Load option_pnl_cube and delta_pnl_cube
option_pnl_3d, _, _, _ = load_cube_as_3d(RESULT_DIR, 'option_pnl_cube')
delta_pnl_3d, _, _, _ = load_cube_as_3d(RESULT_DIR, 'delta_pnl_cube')
decay_3d, _, _, _ = load_cube_as_3d(RESULT_DIR, 'decay_cube')

# Apply inception weighting (same as for pnl_cube)
option_wpnl = (option_pnl_3d * inception_hedged).sum(axis=(1, 2)) * 10000
delta_wpnl = (delta_pnl_3d * inception_hedged).sum(axis=(1, 2)) * 10000
decay_wpnl = (decay_3d * inception_hedged).sum(axis=(1, 2)) * 10000
total_wpnl_check = (pnl_cube_3d * inception_hedged).sum(axis=(1, 2)) * 10000

option_wpnl_s = pd.Series(option_wpnl, index=pnl_dates)
delta_wpnl_s = pd.Series(delta_wpnl, index=pnl_dates)
decay_wpnl_s = pd.Series(decay_wpnl, index=pnl_dates)
total_wpnl_s = pd.Series(total_wpnl_check, index=pnl_dates)

# Component stats
def component_stats(series):
    nz = series[series != 0]
    if len(nz) < 10:
        return {'mean': 0, 'total': 0, 'sharpe': 0}
    return {
        'mean': nz.mean(),
        'total': nz.sum(),
        'sharpe': (nz.mean() / nz.std()) * np.sqrt(256) if nz.std() > 0 else 0
    }

comp_stats = {
    'Option P\\&L': component_stats(option_wpnl_s),
    'Delta P\\&L': component_stats(delta_wpnl_s),
    'Decay': component_stats(decay_wpnl_s),
    'Total': component_stats(total_wpnl_s)
}

def component_to_latex(stats):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{P\&L Component Decomposition (bps)}')
    lines.append(r'\label{tab:pnl_components}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{l|rrr}')
    lines.append(r'\toprule')
    lines.append(r'Component & Mean daily & Sharpe & Total \\')
    lines.append(r'\midrule')
    for name, s in stats.items():
        cells = [name, f"{s['mean']:.2f}", f"{s['sharpe']:.2f}", f"{int(round(s['total'])):,}"]
        lines.append(' & '.join(cells) + r' \\')
    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

component_latex = component_to_latex(comp_stats)

# Stacked area chart of cumulative components
fig, ax = plt.subplots(figsize=(12, 6))
cumul_option = option_wpnl_s.cumsum()
cumul_delta = delta_wpnl_s.cumsum()
cumul_total_comp = total_wpnl_s.cumsum()
ax.plot(cumul_total_comp.index, cumul_total_comp.values, linewidth=2, color='black', label='Total')
ax.plot(cumul_option.index, cumul_option.values, linewidth=1.2, color='blue', label='Option P&L')
ax.plot(cumul_delta.index, cumul_delta.values, linewidth=1.2, color='red', label='Delta P&L')
ax.plot(decay_wpnl_s.cumsum().index, decay_wpnl_s.cumsum().values, linewidth=1, color='orange', linestyle='--', label='Decay')
ax.axhline(0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('Date')
ax.set_ylabel('Cumulative P&L (bps)')
ax.set_title('Cumulative P&L by Component')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_pnl_components.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# --- Concentration Metrics ---
# Per-asset total wpnl contribution
asset_total_wpnl = wpnl_stored.sum(axis=0) * 10000  # sum over dates, per asset
asset_total_wpnl_abs = np.abs(asset_total_wpnl)
total_abs_wpnl = asset_total_wpnl_abs.sum()

# Sort by absolute contribution
asset_names_list = strategy_asset_df['asset_string'].tolist()
asset_contrib = pd.DataFrame({
    'asset': asset_names_list,
    'wpnl': asset_total_wpnl,
    'abs_wpnl': asset_total_wpnl_abs
}).sort_values('abs_wpnl', ascending=False).reset_index(drop=True)

# Top-10 and top-20 share
top10_share = asset_contrib.head(10)['abs_wpnl'].sum() / total_abs_wpnl * 100
top20_share = asset_contrib.head(20)['abs_wpnl'].sum() / total_abs_wpnl * 100

# Herfindahl index
weights = asset_total_wpnl_abs / total_abs_wpnl
hhi = (weights ** 2).sum()
effective_assets = 1 / hhi if hhi > 0 else 0

# Top-10 table
def concentration_to_latex(df, top10_share, top20_share, effective_assets):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Top 10 Assets by $|$P\&L$|$ Contribution}')
    lines.append(r'\label{tab:concentration}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{l|rr}')
    lines.append(r'\toprule')
    lines.append(r'Asset & Total P\&L (bps) & $|$Contribution$|$ (\%) \\')
    lines.append(r'\midrule')
    for _, row in df.head(10).iterrows():
        pct = row['abs_wpnl'] / total_abs_wpnl * 100
        asset_esc = row['asset'].replace('_', r'\_')
        cells = [asset_esc, f"{int(round(row['wpnl'])):,}", f"{pct:.1f}"]
        lines.append(' & '.join(cells) + r' \\')
    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

concentration_latex = concentration_to_latex(asset_contrib, top10_share, top20_share, effective_assets)
concentration_bullets = (
    f"Top 10 assets: {top10_share:.1f}\\% of $|$P\\&L$|$. "
    f"Top 20: {top20_share:.1f}\\%. "
    f"Effective assets (1/HHI): {effective_assets:.0f} of {len(asset_names_list)}."
)

# --- Volatility Regime Conditioning ---
# Load vol_cube
vol_3d, vol_date_df, vol_asset_df, _ = load_cube_as_3d(RESULT_DIR, 'vol_cube')

# Average implied vol across assets and slices per day
daily_avg_vol = np.nanmean(vol_3d, axis=(1, 2))
vol_dates = pd.to_datetime(vol_date_df['date_string'].values)
vol_series = pd.Series(daily_avg_vol, index=vol_dates)
vol_series = vol_series[vol_series > 0]  # exclude zero days

# Define terciles based on expanding window (to avoid look-ahead)
vol_tercile_low = vol_series.expanding().quantile(0.33)
vol_tercile_high = vol_series.expanding().quantile(0.67)

# Align with daily_pnl
common_dates = daily_pnl.index.intersection(vol_series.index)
pnl_aligned = daily_pnl.loc[common_dates]
vol_aligned = vol_series.loc[common_dates]
low_aligned = vol_tercile_low.loc[common_dates]
high_aligned = vol_tercile_high.loc[common_dates]

low_vol_mask = vol_aligned <= low_aligned
high_vol_mask = vol_aligned >= high_aligned
mid_vol_mask = ~low_vol_mask & ~high_vol_mask

regime_stats = {}
for regime, mask in [('Low Vol', low_vol_mask), ('Mid Vol', mid_vol_mask), ('High Vol', high_vol_mask)]:
    regime_pnl = pnl_aligned[mask]
    nz = regime_pnl[regime_pnl != 0]
    if len(nz) > 10:
        regime_stats[regime] = {
            'days': len(nz),
            'mean': nz.mean(),
            'sharpe': (nz.mean() / nz.std()) * np.sqrt(256) if nz.std() > 0 else 0,
            'hit': (nz > 0).sum() / len(nz) * 100,
            'dd': -(nz.cumsum().cummax() - nz.cumsum()).max()
        }
    else:
        regime_stats[regime] = {'days': 0, 'mean': 0, 'sharpe': 0, 'hit': 0, 'dd': 0}

def vol_regime_to_latex(stats):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Performance by Volatility Regime (bps)}')
    lines.append(r'\label{tab:vol_regime}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{l|rrrr}')
    lines.append(r'\toprule')
    lines.append(r'Regime & Days & Mean P\&L & Sharpe & Hit (\%) \\')
    lines.append(r'\midrule')
    for regime in ['Low Vol', 'Mid Vol', 'High Vol']:
        s = stats[regime]
        cells = [regime, f"{s['days']:,}", f"{s['mean']:.2f}", f"{s['sharpe']:.2f}", f"{s['hit']:.1f}"]
        lines.append(' & '.join(cells) + r' \\')
    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

vol_regime_latex = vol_regime_to_latex(regime_stats)

# --- P&L Recalc Error Matrix ---
# Monthly sum of absolute errors: stored wpnl vs computed (inception-weighted hedged)

# Filter out zero error days
wpnl_error_nz = wpnl_error_series[wpnl_error_series > 0]

# Resample to monthly sum of absolute errors
monthly_error = wpnl_error_nz.resample('ME').sum()
monthly_error = monthly_error[monthly_error > 0]  # Remove zero months

error_matrix_df = pd.DataFrame({
    'error': monthly_error,
    'year': monthly_error.index.year,
    'month': monthly_error.index.month
})

error_pivot = error_matrix_df.pivot(index='year', columns='month', values='error')
error_pivot['Total'] = error_pivot.sum(axis=1)
error_pivot = error_pivot.sort_index(ascending=False)

error_month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Total']
error_pivot.columns = error_month_names

# Color scale for error matrix (all values are positive, use red intensity)
def get_error_cell_color(val, max_val):
    if pd.isna(val) or val == 0:
        return 'white'
    intensity = min(val / max_val, 1.0) * 40
    return f'red!{int(intensity)}'

error_month_cols = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
max_error_months = error_pivot[error_month_cols].max().max() if len(error_pivot) > 0 else 1
max_error_total = error_pivot['Total'].max() if len(error_pivot) > 0 else 1

def error_matrix_to_latex(df, max_months, max_total):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{P\&L Recalc Error Matrix: Sum of $|$stored wpnl - computed wpnl$|$ (bps)}')
    lines.append(r'\label{tab:error_matrix}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{r|' + 'r' * 12 + '|r}')
    lines.append(r'\toprule')
    lines.append('Year & ' + ' & '.join(df.columns) + r' \\')
    lines.append(r'\midrule')

    for year, row in df.iterrows():
        cells = [str(year)]
        for col in df.columns:
            val = row[col]
            if pd.isna(val) or val == 0:
                cells.append('')
            else:
                if col == 'Total':
                    color = get_error_cell_color(val, max_total)
                else:
                    color = get_error_cell_color(val, max_months)
                # Format small values with decimals, larger with commas
                if val < 1:
                    val_str = f'{val:.4f}'
                elif val < 10:
                    val_str = f'{val:.2f}'
                else:
                    val_str = f'{val:.1f}'
                cells.append(r'\cellcolor{' + color + '}' + val_str)
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

# Check if there are any errors
if len(error_pivot) > 0 and error_pivot['Total'].sum() > 0:
    error_matrix_latex = error_matrix_to_latex(error_pivot, max_error_months, max_error_total)
else:
    # No errors - create a simple message
    error_matrix_latex = r'''\begin{table}[H]
\centering
\caption{P\&L Recalc Error Matrix: Sum of $|$stored wpnl - computed wpnl$|$ (bps)}
\label{tab:error_matrix}
\textcolor{green!70!black}{\textbf{No errors detected - computed wpnl matches stored wpnl exactly.}}
\end{table}'''

# --- P&L Matrix ---
# Monthly P&L matrix by year

# Filter out zero months
monthly_pnl_nz = monthly_pnl[monthly_pnl != 0]

pnl_matrix_df = pd.DataFrame({
    'pnl': monthly_pnl_nz,
    'year': monthly_pnl_nz.index.year,
    'month': monthly_pnl_nz.index.month
})

pnl_pivot = pnl_matrix_df.pivot(index='year', columns='month', values='pnl')
pnl_pivot['Total'] = pnl_pivot.sum(axis=1)
pnl_pivot = pnl_pivot.sort_index(ascending=False)

month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Total']
pnl_pivot.columns = month_names

def get_cell_color(val, max_abs):
    if pd.isna(val):
        return 'white'
    intensity = min(abs(val) / max_abs, 1.0) * 40
    if val > 0:
        return f'green!{int(intensity)}'
    elif val < 0:
        return f'red!{int(intensity)}'
    else:
        return 'white'

# Separate scaling for months vs Total
month_cols = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
max_abs_months = pnl_pivot[month_cols].abs().max().max()
max_abs_total = pnl_pivot['Total'].abs().max()

def pnl_matrix_to_latex(df, max_abs_months, max_abs_total):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Monthly P\&L Matrix (bps)}')
    lines.append(r'\label{tab:pnl_matrix}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{r|' + 'r' * 12 + '|r}')
    lines.append(r'\toprule')
    lines.append('Year & ' + ' & '.join(df.columns) + r' \\')
    lines.append(r'\midrule')

    for year, row in df.iterrows():
        cells = [str(year)]
        for col in df.columns:
            val = row[col]
            if pd.isna(val):
                cells.append('')
            else:
                if col == 'Total':
                    color = get_cell_color(val, max_abs_total)
                else:
                    color = get_cell_color(val, max_abs_months)
                val_str = f'{int(round(val)):,}'
                cells.append(r'\cellcolor{' + color + '}' + val_str)
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

pnl_matrix_latex = pnl_matrix_to_latex(pnl_pivot, max_abs_months, max_abs_total)

# --- Rolling Hit Ratio ---
# Compute rolling hit ratio (252-day window) for total portfolio
rolling_hit_total = (daily_pnl > 0).rolling(window=252, min_periods=60).mean() * 100

# Plot rolling hit ratio (total)
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(rolling_hit_total.index, rolling_hit_total.values, linewidth=1, color='blue')
ax.axhline(50, color='gray', linestyle='--', alpha=0.5)
mean_hit = rolling_hit_total.mean()
ax.axhline(mean_hit, color='red', linestyle='--', alpha=0.7,
           label=f'Mean: {mean_hit:.1f}%')
ax.set_xlabel('Date')
ax.set_ylabel('Hit Ratio (%)')
ax.set_title('Rolling Hit Ratio - Total Portfolio (252-day rolling)')
# Dynamic y-axis limits with padding
ymin = rolling_hit_total.min() - 3
ymax = rolling_hit_total.max() + 3
ax.set_ylim(ymin, ymax)
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_rolling_hit_total.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# Rolling hit ratio by asset class
fig, ax = plt.subplots(figsize=(12, 6))
all_hit_values = []
for cls in class_name_list + ['Total']:
    if cls in class_daily_pnl:
        pnl = class_daily_pnl[cls]
        pnl_nz = pnl[pnl != 0]
        rolling_hit = (pnl_nz > 0).rolling(window=252, min_periods=60).mean() * 100
        all_hit_values.extend(rolling_hit.dropna().values)
        lw = 2.5 if cls == 'Total' else 1.2
        ax.plot(rolling_hit.index, rolling_hit.values, label=cls,
                color=colors_map.get(cls, 'gray'), linewidth=lw, alpha=0.8)

ax.axhline(50, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('Date')
ax.set_ylabel('Hit Ratio (%)')
ax.set_title('Rolling Hit Ratio by Asset Class (252-day rolling)')
# Dynamic y-axis limits with padding
ymin = min(all_hit_values) - 3
ymax = max(all_hit_values) + 3
ax.set_ylim(ymin, ymax)
ax.legend(loc='lower left')
ax.grid(True, alpha=0.3)
plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_rolling_hit_by_class.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# --- Recovery Length Analysis ---
# For each loss month, count months until cumulative P&L turns positive

def compute_recovery_months(pnl_series):
    """Compute recovery months for each loss month in a P&L series.

    Returns list of recovery lengths (in months) for each loss month.
    None if recovery never happened within the data.
    """
    pnl = pnl_series.dropna()
    pnl = pnl[pnl != 0]  # Remove zero months

    recovery_lengths = []

    for i in range(len(pnl)):
        if pnl.iloc[i] < 0:  # Loss month
            # Track cumulative P&L from this point
            cumsum = pnl.iloc[i]
            months_to_recover = None
            for j in range(i + 1, len(pnl)):
                cumsum += pnl.iloc[j]
                if cumsum >= 0:
                    months_to_recover = j - i
                    break
            recovery_lengths.append((pnl.iloc[i], months_to_recover))

    return recovery_lengths

def avg_recovery_by_threshold(recovery_data, threshold=0):
    """Compute average recovery months for losses exceeding threshold."""
    filtered = [(loss, months) for loss, months in recovery_data
                if loss < -threshold and months is not None]
    if len(filtered) == 0:
        return np.nan, 0
    return np.mean([m for _, m in filtered]), len(filtered)

# Compute recovery for each asset class using monthly P&L
monthly_by_class = {}
for cls in sorted(class_daily_pnl.keys()):
    monthly_by_class[cls] = class_daily_pnl[cls].resample('ME').sum()
    monthly_by_class[cls] = monthly_by_class[cls][monthly_by_class[cls] != 0]

recovery_by_class = {}
for cls in sorted(monthly_by_class.keys()):
    recovery_by_class[cls] = compute_recovery_months(monthly_by_class[cls])

# Build recovery table
def build_recovery_table(recovery_data, thresholds=[0, 100, 200, 400]):
    rows = []
    for cls in sorted([c for c in recovery_data.keys() if c != 'Total']):
        row = {'class': cls}
        for thresh in thresholds:
            avg, n = avg_recovery_by_threshold(recovery_data[cls], thresh)
            row[f't{thresh}'] = avg
            row[f'n{thresh}'] = n
        rows.append(row)

    # Add "Total" row
    if 'Total' in recovery_data:
        row = {'class': 'Total'}
        for thresh in thresholds:
            avg, n = avg_recovery_by_threshold(recovery_data['Total'], thresh)
            row[f't{thresh}'] = avg
            row[f'n{thresh}'] = n
        rows.append(row)

    return pd.DataFrame(rows)

recovery_df = build_recovery_table(recovery_by_class)

# Generate LaTeX for recovery table
def recovery_table_to_latex(dataframe):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Average Months to Recovery by Asset Class}')
    lines.append(r'\label{tab:recovery}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{lrrrr}')
    lines.append(r'\toprule')
    lines.append(r'Class & Any Loss & $>$100 bps & $>$200 bps & $>$400 bps \\')
    lines.append(r'\midrule')

    for _, row in dataframe.iterrows():
        name = str(row['class']).replace('_', r'\_')
        t0 = f"{row['t0']:.1f}" if pd.notna(row['t0']) else '--'
        t100 = f"{row['t100']:.1f}" if pd.notna(row['t100']) else '--'
        t200 = f"{row['t200']:.1f}" if pd.notna(row['t200']) else '--'
        t400 = f"{row['t400']:.1f}" if pd.notna(row['t400']) else '--'

        if row['class'] == 'Total':
            lines.append(r'\midrule')
            lines.append(f'\\textbf{{{name}}} & \\textbf{{{t0}}} & \\textbf{{{t100}}} & \\textbf{{{t200}}} & \\textbf{{{t400}}} \\\\')
        else:
            lines.append(f'{name} & {t0} & {t100} & {t200} & {t400} \\\\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

recovery_table_latex = recovery_table_to_latex(recovery_df)

# Also create a table with sample counts
def recovery_count_to_latex(dataframe):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Number of Loss Months by Threshold}')
    lines.append(r'\label{tab:recovery_count}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{lrrrr}')
    lines.append(r'\toprule')
    lines.append(r'Class & Any Loss & $>$100 bps & $>$200 bps & $>$400 bps \\')
    lines.append(r'\midrule')

    for _, row in dataframe.iterrows():
        name = str(row['class']).replace('_', r'\_')
        n0 = int(row['n0']) if pd.notna(row['n0']) else 0
        n100 = int(row['n100']) if pd.notna(row['n100']) else 0
        n200 = int(row['n200']) if pd.notna(row['n200']) else 0
        n400 = int(row['n400']) if pd.notna(row['n400']) else 0

        if row['class'] == 'Total':
            lines.append(r'\midrule')
            lines.append(f'\\textbf{{{name}}} & \\textbf{{{n0}}} & \\textbf{{{n100}}} & \\textbf{{{n200}}} & \\textbf{{{n400}}} \\\\')
        else:
            lines.append(f'{name} & {n0} & {n100} & {n200} & {n400} \\\\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

recovery_count_latex = recovery_count_to_latex(recovery_df)

# --- Recovery Enhancements ---
# Compute max recovery, % never recovered, and histogram for Total
total_recovery = recovery_by_class.get('Total', [])
total_recovered = [(loss, months) for loss, months in total_recovery if months is not None]
total_never_recovered = [(loss, months) for loss, months in total_recovery if months is None]

max_recovery_months = max([m for _, m in total_recovered]) if total_recovered else 0
pct_never_recovered = len(total_never_recovered) / len(total_recovery) * 100 if total_recovery else 0
total_loss_months = len(total_recovery)

recovery_enhancement_bullets = (
    f"\\item Total loss months: {total_loss_months}\n"
    f"\\item Maximum recovery: {max_recovery_months} months\n"
    f"\\item Never recovered within data: {len(total_never_recovered)} ({pct_never_recovered:.1f}\\%)"
)

# Histogram of recovery lengths
recovery_lengths_all = [m for _, m in total_recovered if m is not None]
if len(recovery_lengths_all) > 0:
    fig, ax = plt.subplots(figsize=(10, 5))
    max_len = max(recovery_lengths_all)
    bins = range(0, max_len + 2)
    ax.hist(recovery_lengths_all, bins=bins, edgecolor='black', alpha=0.7, color='steelblue')
    ax.axvline(np.mean(recovery_lengths_all), color='red', linestyle='--', linewidth=2,
               label=f'Mean: {np.mean(recovery_lengths_all):.1f} months')
    ax.axvline(np.median(recovery_lengths_all), color='green', linestyle='--', linewidth=2,
               label=f'Median: {np.median(recovery_lengths_all):.0f} months')
    ax.set_xlabel('Months to Recovery')
    ax.set_ylabel('Frequency')
    ax.set_title('Distribution of Recovery Lengths (Total Portfolio)')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    fig.savefig(FIGURE_DIR + 'result_recovery_hist.pdf', dpi=150, bbox_inches='tight')
    plt.close(fig)
    recovery_hist_exists = True
else:
    recovery_hist_exists = False

# --- Entries Matrix ---
# Load entry_cube and sum across slice axis
entry_cube = pd.read_parquet(f"{RESULT_DIR}/entry_cube.parquet")
entry_cube_date = pd.read_parquet(f"{RESULT_DIR}/entry_cube_date.parquet")

# Sum across slices to get total entries per date
entry_by_date = entry_cube.groupby('date')['value'].sum().reset_index()
entry_by_date = entry_by_date.merge(entry_cube_date, on='date')
entry_by_date['date_dt'] = pd.to_datetime(entry_by_date['date_string'])
entry_by_date['year'] = entry_by_date['date_dt'].dt.year
entry_by_date['month'] = entry_by_date['date_dt'].dt.month

# Sum entries per year/month
entry_counts = entry_by_date.groupby(['year', 'month'])['value'].sum().reset_index(name='count')

# Pivot to matrix form
entry_pivot = entry_counts.pivot(index='year', columns='month', values='count')
entry_pivot = entry_pivot.sort_index(ascending=False)

# Rename columns to month names
month_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
entry_pivot.columns = [month_names_short[m-1] for m in entry_pivot.columns]

# Compute median for color coding
all_entry_values = entry_pivot.values.flatten()
all_entry_values = all_entry_values[~np.isnan(all_entry_values)]
entry_median = np.median(all_entry_values)
n_total_entries = int(all_entry_values.sum())

def get_matrix_cell_color(val, median, all_vals):
    if pd.isna(val):
        return 'white'
    max_dist = max(abs(all_vals.max() - median), abs(all_vals.min() - median))
    if max_dist == 0:
        return 'white'
    intensity = min(abs(val - median) / max_dist, 1.0) * 30
    if val > median:
        return f'green!{int(intensity)}'
    elif val < median:
        return f'red!{int(intensity)}'
    else:
        return 'white'

def entries_matrix_to_latex(df, median, all_vals):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Entry Count by Month (sum of entry\_cube)}')
    lines.append(r'\label{tab:entries}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{r|' + 'r' * len(df.columns) + '}')
    lines.append(r'\toprule')
    lines.append('Year & ' + ' & '.join(df.columns) + r' \\')
    lines.append(r'\midrule')

    for year, row in df.iterrows():
        cells = [str(year)]
        for col in df.columns:
            val = row[col]
            if pd.isna(val):
                cells.append('')
            else:
                color = get_matrix_cell_color(val, median, all_vals)
                val_str = f'{int(val):,}'
                cells.append(r'\cellcolor{' + color + '}' + val_str)
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

entries_matrix_latex = entries_matrix_to_latex(entry_pivot, entry_median, all_entry_values)

# --- Exits Matrix ---
# Load expiry_cube and sum across slice axis
expiry_cube = pd.read_parquet(f"{RESULT_DIR}/expiry_cube.parquet")
expiry_cube_date = pd.read_parquet(f"{RESULT_DIR}/expiry_cube_date.parquet")

# Sum across slices to get total exits per date
exit_by_date = expiry_cube.groupby('date')['value'].sum().reset_index()
exit_by_date = exit_by_date.merge(expiry_cube_date, on='date')
exit_by_date['date_dt'] = pd.to_datetime(exit_by_date['date_string'])
exit_by_date['year'] = exit_by_date['date_dt'].dt.year
exit_by_date['month'] = exit_by_date['date_dt'].dt.month

# Sum exits per year/month
exit_counts = exit_by_date.groupby(['year', 'month'])['value'].sum().reset_index(name='count')

# Pivot to matrix form
exit_pivot = exit_counts.pivot(index='year', columns='month', values='count')
exit_pivot = exit_pivot.sort_index(ascending=False)

# Rename columns to month names
exit_pivot.columns = [month_names_short[m-1] for m in exit_pivot.columns]

# Compute median for color coding
all_exit_values = exit_pivot.values.flatten()
all_exit_values = all_exit_values[~np.isnan(all_exit_values)]
exit_median = np.median(all_exit_values)
n_total_exits = int(all_exit_values.sum())

def exits_matrix_to_latex(df, median, all_vals):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Exit Count by Month (sum of expiry\_cube)}')
    lines.append(r'\label{tab:exits}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{r|' + 'r' * len(df.columns) + '}')
    lines.append(r'\toprule')
    lines.append('Year & ' + ' & '.join(df.columns) + r' \\')
    lines.append(r'\midrule')

    for year, row in df.iterrows():
        cells = [str(year)]
        for col in df.columns:
            val = row[col]
            if pd.isna(val):
                cells.append('')
            else:
                color = get_matrix_cell_color(val, median, all_vals)
                val_str = f'{int(val):,}'
                cells.append(r'\cellcolor{' + color + '}' + val_str)
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

exits_matrix_latex = exits_matrix_to_latex(exit_pivot, exit_median, all_exit_values)

# --- Entries - Exits Matrix ---
# Compute difference: entries - exits per month
# Both pivots have same structure (year x month), align them
diff_pivot = entry_pivot.copy()
for col in diff_pivot.columns:
    if col in exit_pivot.columns:
        diff_pivot[col] = entry_pivot[col].subtract(exit_pivot[col], fill_value=0)

# For color scaling: positive = more entries than exits (green), negative = more exits (red)
all_diff_values = diff_pivot.values.flatten()
all_diff_values = all_diff_values[~np.isnan(all_diff_values)]
max_abs_diff = np.abs(all_diff_values).max() if len(all_diff_values) > 0 else 1

def diff_matrix_to_latex(df, max_abs):
    lines = []
    lines.append(r'\begin{table}[H]')
    lines.append(r'\centering')
    lines.append(r'\caption{Entries - Exits by Month}')
    lines.append(r'\label{tab:diff_matrix}')
    lines.append(r'\small')
    lines.append(r'\begin{tabular}{r|' + 'r' * len(df.columns) + '}')
    lines.append(r'\toprule')
    lines.append('Year & ' + ' & '.join(df.columns) + r' \\')
    lines.append(r'\midrule')

    for year, row in df.iterrows():
        cells = [str(year)]
        for col in df.columns:
            val = row[col]
            if pd.isna(val):
                cells.append('')
            else:
                # Positive = more entries (green), negative = more exits (red)
                intensity = min(abs(val) / max_abs, 1.0) * 40 if max_abs > 0 else 0
                if val > 0:
                    color = f'green!{int(intensity)}'
                elif val < 0:
                    color = f'red!{int(intensity)}'
                else:
                    color = 'white'
                val_str = f'{int(val):+,}' if val != 0 else '0'
                cells.append(r'\cellcolor{' + color + '}' + val_str)
        lines.append(' & '.join(cells) + r' \\')

    lines.append(r'\bottomrule')
    lines.append(r'\end{tabular}')
    lines.append(r'\end{table}')
    return '\n'.join(lines)

diff_matrix_latex = diff_matrix_to_latex(diff_pivot, max_abs_diff)

# --- Entry - Exit Reconciliation ---
# Load cubes for reconciliation
entry_cube_3d, entry_date_df, _, _ = load_cube_as_3d(RESULT_DIR, 'entry_cube')
exit_cube_3d, _, _, _ = load_cube_as_3d(RESULT_DIR, 'expiry_cube')
ewc_cube_3d, _, _, _ = load_cube_as_3d(RESULT_DIR, 'entry_weight_cube')

recon_dates = pd.to_datetime(entry_date_df['date_string'].values)

# Daily counts
daily_entries_recon = (entry_cube_3d > 0).sum(axis=(1, 2))
daily_exits_recon = (exit_cube_3d > 0).sum(axis=(1, 2))
cumsum_entries_exits = np.cumsum(daily_entries_recon - daily_exits_recon)
live_straddles_recon = (ewc_cube_3d != 0).sum(axis=(1, 2))

# Key insight: live_straddles = cumsum + exits_today
# Because on exit day, the straddle is still "live" (ewc != 0)
implied_live = cumsum_entries_exits + daily_exits_recon
recon_match = np.allclose(implied_live, live_straddles_recon)

# Create reconciliation stats
recon_stats = {
    'total_entries': int(daily_entries_recon.sum()),
    'total_exits': int(daily_exits_recon.sum()),
    'net_entries_exits': int(daily_entries_recon.sum() - daily_exits_recon.sum()),
    'max_cumsum': int(cumsum_entries_exits.max()),
    'min_live': int(live_straddles_recon.min()),
    'max_live': int(live_straddles_recon.max()),
    'mean_live': float(live_straddles_recon.mean()),
    'recon_match': recon_match,
    'max_recon_diff': int(np.abs(implied_live - live_straddles_recon).max())
}

# Create time series for plotting
recon_series = pd.DataFrame({
    'cumsum_entries_exits': cumsum_entries_exits,
    'live_straddles': live_straddles_recon,
    'daily_exits': daily_exits_recon
}, index=recon_dates)

# Plot reconciliation
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# Top: cumsum vs live straddles
ax1 = axes[0]
ax1.plot(recon_series.index, recon_series['live_straddles'],
         label='Live Straddles (ewc ≠ 0)', linewidth=1.5, color='blue')
ax1.plot(recon_series.index, recon_series['cumsum_entries_exits'],
         label='Cumsum(Entries - Exits)', linewidth=1, color='red', alpha=0.7)
ax1.set_ylabel('Count')
ax1.set_title('Live Straddles vs Cumulative Entries - Exits')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Bottom: the difference (should equal daily_exits)
ax2 = axes[1]
diff_series = recon_series['live_straddles'] - recon_series['cumsum_entries_exits']
ax2.plot(recon_series.index, diff_series, label='Live - Cumsum (= Exits Today)',
         linewidth=1, color='green')
ax2.plot(recon_series.index, recon_series['daily_exits'],
         label='Daily Exits', linewidth=1, color='orange', alpha=0.5)
ax2.set_xlabel('Date')
ax2.set_ylabel('Count')
ax2.set_title('Difference = Daily Exits (straddles exit on their last live day)')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
fig.savefig(FIGURE_DIR + 'result_entry_exit_recon.pdf', dpi=150, bbox_inches='tight')
plt.close(fig)

# --- Executive Summary Variables ---
exec_sharpe = pnl_stats_t.loc['sharpe', 'Total']
exec_mean_pnl = pnl_stats_t.loc['mean_pnl', 'Total']
exec_hit_ratio = pnl_stats_t.loc['hit_ratio', 'Total']
exec_drawdown = int(round(pnl_stats_t.loc['drawdown', 'Total']))
exec_cor_ls = pnl_stats_t.loc['cor_ls', 'Total'] * 100
@

\begin{document}
\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage

\section{Executive Summary}

\begin{itemize}
\item \textbf{Assets:} \Sexpr{py$n_assets} \quad \textbf{Dates:} \Sexpr{py$n_dates} (\Sexpr{py$date_min} to \Sexpr{py$date_max})
\item \textbf{Sharpe (ann.):} \Sexpr{sprintf("%.2f", py$exec_sharpe)}
\item \textbf{Mean daily P\&L:} \Sexpr{sprintf("%.2f", py$exec_mean_pnl)} bps
\item \textbf{Hit Ratio:} \Sexpr{sprintf("%.1f", py$exec_hit_ratio)}\%
\item \textbf{Max Drawdown:} \Sexpr{format(py$exec_drawdown, big.mark=",")} bps
\item \textbf{Long/Short Correlation:} \Sexpr{sprintf("%.1f", py$exec_cor_ls)}\%
\item \textbf{Signal types:} \Sexpr{py$n_signals} \quad \textbf{Stagger weights:} \Sexpr{py$n_pnl_slices}
\end{itemize}

\newpage

\section{Data Summary}

This document presents the structure and contents of backtest results from:

\texttt{result\_nick\_20260202\_stagger1\_f1d8}

\begin{itemize}
\item \textbf{Total files:} \Sexpr{py$n_files} parquet files
\item \textbf{Total size:} \Sexpr{sprintf("%.1f", py$total_size_mb)} MB
\item \textbf{Assets:} \Sexpr{py$n_assets}
\item \textbf{Dates:} \Sexpr{py$n_dates} (\Sexpr{py$date_min} to \Sexpr{py$date_max})
\item \textbf{Strategy slices:} \Sexpr{py$n_strategy_slices}
\item \textbf{Signal types:} \Sexpr{py$n_signals}
\item \textbf{Stagger weights:} \Sexpr{py$n_pnl_slices}
\end{itemize}

\newpage
\section{Backtest Statistics}

<<stats_table, engine='python', results='asis'>>=
print(stats_latex)
@

\newpage
\section{Signal Hedged Statistics}

P\&L statistics for each signal-residual combination. Each table shows 29 statistics across asset classes.

<<signal_hedged_stats, engine='python', results='asis'>>=
print(signal_hedged_all_latex)
@

\newpage
\section{Signal Quality Ranking}

Signals ranked by Sharpe ratio (``all'' column, descending). Sharpe computed from hedged signal weights.

<<signal_ranking, engine='python', results='asis'>>=
print(signal_ranking_latex)
@

\newpage
\section{P\&L Stats}

Summary statistics by asset class computed from daily P\&L.

<<pnl_stats_out, engine='python', results='asis'>>=
print(pnl_stats_latex)
@

<<tail_risk_out, engine='python', results='asis'>>=
print(tail_risk_latex)
@

\newpage
\section{Annual Performance}

Year-by-year Sharpe ratio, mean daily P\&L, hit ratio, maximum drawdown, and total P\&L.

<<annual_perf, engine='python', results='asis'>>=
print(annual_perf_latex)
@

\newpage
\section{Reference P\&L Stats}

P\&L statistics computed using the reference implementation weights (hedged\_01 from reference cube).

<<ref_pnl_stats, engine='python', results='asis'>>=
print(ref_stats_latex)
@

\begin{itemize}
<<ref_wpnl_bullet, engine='python', results='asis'>>=
print(f"\\item {ref_wpnl_bullet}")
@
\end{itemize}

\newpage
\section{P\&L Recalc Error}

This matrix shows the sum of absolute differences between stored wpnl (from strategy cube) and computed wpnl (inception-weighted hedged $\times$ pnl). All values should be zero or negligible, confirming the inception weighting algorithm is correct.

<<error_matrix_out, engine='python', results='asis'>>=
print(error_matrix_latex)
@

\newpage
\section{P\&L Component Decomposition}

Breakdown of total P\&L into option P\&L (gamma + theta + vega) and delta hedge P\&L.

<<pnl_components_out, engine='python', results='asis'>>=
print(component_latex)
@

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_pnl_components.pdf}
\caption{Cumulative P\&L by Component (option, delta, decay)}
\end{figure}

\newpage
\section{Concentration}

How concentrated is P\&L across assets.

<<concentration_out, engine='python', results='asis'>>=
print(concentration_latex)
print(f"\\noindent {concentration_bullets}")
@

\newpage
\section{Volatility Regime}

Performance conditioned on implied volatility level (expanding-window terciles to avoid look-ahead).

<<vol_regime_out, engine='python', results='asis'>>=
print(vol_regime_latex)
@

\newpage
\section{P\&L Matrix}

Monthly P\&L by year and month. Green indicates gains, red indicates losses. Color intensity reflects magnitude (months scaled among months, years scaled among years).

<<pnl_matrix_out, engine='python', results='asis'>>=
print(pnl_matrix_latex)
@

\newpage
\section{Cumulative P\&L}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_cumulative_pnl.pdf}
\caption{Cumulative Strategy P\&L with drawdown (bottom panel)}
\end{figure}

\newpage
\section{Drawdown by Asset Class}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_drawdown_by_class.pdf}
\caption{Drawdown by Asset Class}
\end{figure}

\newpage
\section{Monthly P\&L Distribution}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{result_monthly_hist.pdf}
\caption{Distribution of Monthly P\&L}
\end{figure}

\newpage
\section{Cumulative P\&L by Asset Class}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_cumulative_by_class.pdf}
\caption{Cumulative P\&L by Asset Class}
\end{figure}

\newpage
\section{Rolling Volatility by Asset Class}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_rolling_vol_by_class.pdf}
\caption{Rolling Volatility by Asset Class (EMA of $|$P\&L$|$, 364/365 decay)}
\end{figure}

\newpage
\section{Realized Correlation: Long vs Short P\&L}

Rolling correlation between P\&L from long positions (positive weights) and short positions (negative weights).

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_long_short_corr.pdf}
\caption{Realized Correlation: Long vs Short P\&L (252-day rolling window)}
\end{figure}

\newpage
\section{Realized Correlation by Asset Class}

Rolling correlation between long and short P\&L within each asset class.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_long_short_corr_by_class.pdf}
\caption{Realized Correlation: Long vs Short P\&L by Asset Class (252-day rolling)}
\end{figure}

\newpage
\section{Cross-Asset-Class Correlation}

Pairwise correlation of daily P\&L between asset classes (excluding zero-activity days).

<<cross_class_corr_out, engine='python', results='asis'>>=
print(cross_class_corr_latex)
@

\newpage
\section{Rolling Hit Ratio}

Rolling 252-day hit ratio (percentage of winning days) for the total portfolio.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_rolling_hit_total.pdf}
\caption{Rolling Hit Ratio - Total Portfolio (252-day rolling window)}
\end{figure}

\newpage
\section{Rolling Hit Ratio by Asset Class}

Rolling 252-day hit ratio by asset class.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_rolling_hit_by_class.pdf}
\caption{Rolling Hit Ratio by Asset Class (252-day rolling window)}
\end{figure}

\newpage
\section{Outcomes by Period Length}

Statistics of rolling cumulative P\&L over windows of different lengths. All values in bps.

<<outcomes_by_period, engine='python', results='asis'>>=
# Compute outcomes by period using ref_daily_pnl_by_class
# Period lengths in trading days
periods = {
    '1 week': 5,
    '1 month': 21,
    '3 months': 63,
    '6 months': 126,
    '1 year': 252
}

# Asset class order
cols = ['commodity', 'equity', 'fx', 'rates', 'stonks', 'all']
col_headers = ['Cmdty', 'Eqty', 'FX', 'Rates', 'Stnks', 'All']

# Compute all stats for each period and class
stats_min = {}
stats_max = {}
stats_median = {}
stats_mean = {}

for period_name, window in periods.items():
    stats_min[period_name] = {}
    stats_max[period_name] = {}
    stats_median[period_name] = {}
    stats_mean[period_name] = {}
    for cls in cols:
        if cls in ref_daily_pnl_by_class:
            pnl = ref_daily_pnl_by_class[cls]
            rolling_sum = pnl.rolling(window=window, min_periods=window).sum()
            stats_min[period_name][cls] = rolling_sum.min()
            stats_max[period_name][cls] = rolling_sum.max()
            stats_median[period_name][cls] = rolling_sum.median()
            stats_mean[period_name][cls] = rolling_sum.mean()
        else:
            stats_min[period_name][cls] = 0
            stats_max[period_name][cls] = 0
            stats_median[period_name][cls] = 0
            stats_mean[period_name][cls] = 0

# Generate combined table with statistics as separators
all_stats = [
    ('Minimum (worst case)', stats_min),
    ('Maximum (best case)', stats_max),
    ('Median', stats_median),
    ('Mean', stats_mean)
]

lines = []
lines.append(r'\begin{table}[H]')
lines.append(r'\centering')
lines.append(r'\caption{Outcomes by Period Length (bps)}')
lines.append(r'\label{tab:outcomes_by_period}')
lines.append(r'\small')
lines.append(r'\begin{tabular}{l|rrrrr|r}')
lines.append(r'\toprule')
lines.append('Period & ' + ' & '.join(col_headers) + r' \\')

for stat_name, stats_dict in all_stats:
    lines.append(r'\midrule')
    lines.append(r' & \multicolumn{6}{c}{' + stat_name + r'} \\')
    lines.append(r'\midrule')
    for period_name in periods.keys():
        cells = [period_name]
        for cls in cols:
            val = stats_dict[period_name].get(cls, 0)
            cells.append(f'{int(round(val)):,}')
        lines.append(' & '.join(cells) + r' \\')

lines.append(r'\bottomrule')
lines.append(r'\end{tabular}')
lines.append(r'\end{table}')

print('\n'.join(lines))
@

\newpage
\section{Recovery Length}

This section analyzes how many months it takes to recover from a loss-making month. Recovery is defined as the number of months until cumulative P\&L (starting from the loss month) turns non-negative.

<<recovery_table, engine='python', results='asis'>>=
print(recovery_table_latex)
@

<<recovery_count, engine='python', results='asis'>>=
print(recovery_count_latex)
@

\begin{itemize}
<<recovery_enhance, engine='python', results='asis'>>=
print(recovery_enhancement_bullets)
@
\end{itemize}

<<recovery_hist_fig, engine='python', results='asis'>>=
if recovery_hist_exists:
    print(r'\begin{figure}[H]')
    print(r'\centering')
    print(r'\includegraphics[width=0.85\textwidth]{result_recovery_hist.pdf}')
    print(r'\caption{Distribution of Recovery Lengths (Total Portfolio)}')
    print(r'\end{figure}')
@

\newpage
\section{Entries}

Entry count by month. Each entry is a unique (entry\_date, Underlying, Entry) combination. Green indicates above-median count, red indicates below-median count. Total entries: \Sexpr{format(py$n_total_entries, big.mark=",")}.

<<entries_matrix, engine='python', results='asis'>>=
print(entries_matrix_latex)
@

\newpage
\section{Exits}

Exit count by month. Each exit is a unique (expiry\_date, Underlying, Expiry) combination. Green indicates above-median count, red indicates below-median count. Total exits: \Sexpr{format(py$n_total_exits, big.mark=",")}.

<<exits_matrix, engine='python', results='asis'>>=
print(exits_matrix_latex)
@

\newpage
\section{Entries - Exits}

Difference between entries and exits per month (entries minus exits). Positive values (green) indicate more entries than exits, negative values (red) indicate more exits than entries.

<<diff_matrix, engine='python', results='asis'>>=
print(diff_matrix_latex)
@

\newpage
\section{Entry - Exit Reconciliation}

The entries-exits difference can appear confusing because large positive values one month are often followed by large negative values. Here's the key insight:

\textbf{On the exit day, a straddle is both ``live'' (ewc $\neq$ 0) AND counted as exited.}

This means:
\begin{itemize}
\item \textbf{Live Straddles} = Cumsum(Entries - Exits) + Exits Today
\item The cumulative sum tracks straddles that have entered but not yet reached their exit day
\item On the exit day itself, the straddle is still active (still has P\&L) but is also counted as an exit
\end{itemize}

\subsection{Reconciliation Statistics}

\begin{itemize}
\item Total Entries: \Sexpr{format(py$recon_stats$total_entries, big.mark=",")}
\item Total Exits: \Sexpr{format(py$recon_stats$total_exits, big.mark=",")}
\item Net (should be 0): \Sexpr{py$recon_stats$net_entries_exits}
\item Max Cumsum(Entries - Exits): \Sexpr{format(py$recon_stats$max_cumsum, big.mark=",")}
\item Live Straddles: min=\Sexpr{py$recon_stats$min_live}, max=\Sexpr{format(py$recon_stats$max_live, big.mark=",")}, mean=\Sexpr{sprintf("%.0f", py$recon_stats$mean_live)}
\item Reconciliation Match: \Sexpr{if(py$recon_stats$recon_match) "Yes" else "No"} (max diff = \Sexpr{py$recon_stats$max_recon_diff})
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{result_entry_exit_recon.pdf}
\caption{Top: Live straddles vs cumulative entries-exits. Bottom: The difference equals daily exits, confirming that straddles are ``live'' on their exit day.}
\end{figure}

\newpage
\appendix

\section{Files Overview}

<<files_table, engine='python', results='asis'>>=
print(files_latex)
@

\newpage
\section{File Contents}

Color-coded reference for all parquet files. Row counts shown as K (thousands) or M (millions).

<<file_contents_table, engine='python', results='asis'>>=
print(file_contents_latex)
@

\subsection{Key File Details}

\textbf{Cubes:} All cubes stored in long format with columns (date, asset, dim3, value). Reshape to 3D with \texttt{values.reshape((n\_dates, n\_assets, n\_dim3), order='C')}.

\textbf{P\&L Cubes:} All share dimensions 9,854 dates $\times$ 180 assets $\times$ 10 slices (stagger weights). The \texttt{entry\_weight\_cube} contains unsigned stagger weights (0.25, 0.333) propagated for straddle life.

\textbf{Strategy Cube Slices:} asset\_class, sub\_class, pnl, cap, liquidity, winsorized\_pnl, rank\_rescale, rank\_sum, rank\_sum\_median, rank\_centered, conviction, is\_live, normalized\_signal, risk\_weight, risk\_signal, class\_weight, class\_signal, beta, norm, redistribute, \textbf{hedged} (signed weights), \textbf{wpnl} (weighted P\&L).

\textbf{Signal Types:} Combine correlation (pearson, tau, one, sign) with metric (rsi, calmar, sign, streak, range) = 20 signals.

\textbf{backtest.parquet columns:} Underlying, straddle, Class, Entry, Expiry, EntryWeight, entry\_date, expiry\_date, date, px, fsw, pva, strike, vol, mv, option\_pnl, delta\_pnl, pnl, vega, decay, spot, bidask, delta, RiskAdj, WeightCap, auxUnderlying.

\newpage
\section{Cube Structure}

All cubes are stored in long format with columns:
\begin{itemize}
\item \texttt{date} -- integer index (1-based)
\item \texttt{asset} -- integer index (1-based)
\item \texttt{\{dim3\}} -- third dimension index (varies: slice, result, signal, residual)
\item \texttt{value} -- the data value
\end{itemize}

Each cube has companion files mapping indices to strings:
\begin{itemize}
\item \texttt{\{cube\}\_date.parquet}: date\_string, date
\item \texttt{\{cube\}\_asset.parquet}: asset\_string, asset
\item \texttt{\{cube\}\_\{dim3\}.parquet}: \{dim3\}\_string, \{dim3\}
\end{itemize}

<<cubes_table, engine='python', results='asis'>>=
print(cubes_latex)
@

\newpage
\section{Assets}

<<assets_table, engine='python', results='asis'>>=
print(assets_latex)
@

\newpage
\section{Strategy Cube Slices}

The strategy cube contains \Sexpr{py$n_strategy_slices} result types:

<<strategy_slices, engine='python', results='asis'>>=
print(strategy_slices_latex)
@

\section{Signal Types}

<<signals_table, engine='python', results='asis'>>=
print(signals_latex)
@

\section{PnL Cube Slices}

<<pnl_slices, engine='python', results='asis'>>=
print(pnl_slices_latex)
@

\section{Reference Cube Slices}

<<reference_slices, engine='python', results='asis'>>=
print(reference_slices_latex)
@

\end{document}
